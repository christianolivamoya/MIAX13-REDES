{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoFZV1kBZkAi"
   },
   "source": [
    "<font color=\"#CA3532\"><h1 align=\"left\">Inteligencia Artificial Aplicada a la Bolsa (MIAX-13)</h1></font>\n",
    "<font color=\"#5b5a59\"><h2 align=\"left\">Extensión del modelo de regresión lineal a una dimensión arbitraria</h2></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74sT4NIPYtiy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki2-Ji49VcIG"
   },
   "source": [
    "# Datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkQ5ae7LZ-zc"
   },
   "source": [
    "Generación de los datos del problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3__RYeMyZyEg"
   },
   "outputs": [],
   "source": [
    "# Parametros:\n",
    "d = 5 # Dimension del problema\n",
    "w = np.random.randn(1, d)\n",
    "b = 1.0\n",
    "xmin = 0.0\n",
    "xmax = 10.0\n",
    "noise = 1.0\n",
    "n = 1000\n",
    "\n",
    "# Datos del problema generados al azar:\n",
    "x = xmin + np.random.rand(n, d)*(xmax - xmin)\n",
    "t0 = np.dot(x, w.T) + b\n",
    "t = t0 + np.random.randn(n, 1)*noise\n",
    "tmin = np.min(t)\n",
    "tmax = np.max(t)\n",
    "\n",
    "# Distribucion de las dos primeras variables:\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(x[:, 0], x[:, 1], 'o')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.subplot(122)\n",
    "\n",
    "# Grafica de t frente a t0:\n",
    "plt.plot(t0, t, 'o')\n",
    "plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"t0\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.show()\n",
    "\n",
    "# Error esperado:\n",
    "e = np.sum((t-t0)*(t-t0))\n",
    "print(\"Error esperado = %f\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nax9A91aaNc0"
   },
   "source": [
    "Forma de los vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGYdnMLDZ4nG"
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRgATzqpad5m"
   },
   "source": [
    "Modelo de regresión lineal con los parámetros inicializados al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrbCLsvPaTRI"
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(1, d)\n",
    "b = np.random.randn()\n",
    "\n",
    "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
    "y = np.dot(x, w.T) + b\n",
    "\n",
    "# Grafica de y frente a t:\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(t, y, 'o')\n",
    "plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Error:\n",
    "e = np.sum((y-t)*(y-t))\n",
    "print(\"Error = %f\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad_HnpDMas0N"
   },
   "source": [
    "Entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAWYGrCDah2I"
   },
   "outputs": [],
   "source": [
    "nepocas = 64\n",
    "eta = 0.000005\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "k = 1\n",
    "error = []\n",
    "for epoch in range(nepocas):\n",
    "    y = np.dot(x, w.T) + b\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula el error:\n",
    "    e = 0 # TO-DO actualiza el error\n",
    "    #----------------------------------------------------------\n",
    "    error.append(e)\n",
    "\n",
    "    if epoch%4 == 0:\n",
    "        plt.subplot(4, 4, k)\n",
    "        plt.plot(t, y, 'o')\n",
    "        plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
    "        plt.grid(True)\n",
    "        plt.title(\"epoca = %d, err = %.2f\" % (epoch, e))\n",
    "        k += 1\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula los gradientes y actualiza los parametros:\n",
    "\n",
    "    b = b # TO-DO actualiza los parámetros\n",
    "    w = w # TO-DO actualiza los parámetros\n",
    "    #----------------------------------------------------------\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93E3pGM5a51W"
   },
   "source": [
    "Error frente a número de épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzR5BJdWaubX"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(nepocas), error)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uwP6KLQc2qn"
   },
   "source": [
    "# California Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PXIq1nYRckw"
   },
   "source": [
    "California Housing contiene información sobre viviendas en distintas áreas de California, basada en el censo de 1990. El objetivo consiste en predecir el valor medio de las viviendas en cada área. Los atributos son los siguientes:\n",
    "\n",
    "* **MedInc**: Ingreso medio de los hogares dentro de un área\n",
    "\n",
    "* **HouseAge**: Edad media de una vivienda dentro de un área\n",
    "\n",
    "* **AveRooms**: Promedio de habitaciones en una vivienda dentro de un área\n",
    "\n",
    "* **AveBedrms**: Promedio de dormitorios en una vivienda dentro de un área\n",
    "\n",
    "* **Population**: Población en el área\n",
    "\n",
    "* **AveOccup**: Tamaño de familia promedio en el área\n",
    "\n",
    "* **Latitude**: Latitud\n",
    "\n",
    "* **Longitude**: Longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgfkFCxOc4jR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "x, t = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvOJX9p8Rckx"
   },
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSFaYz_nRckx"
   },
   "source": [
    "Separamos en train - test para evaluar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4i0MHdGRckx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"TRAIN\", x_train.shape, y_train.shape)\n",
    "print(\"TEST\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Guhb2FZlSqSt"
   },
   "source": [
    "Estandarizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKz0eODfSvOJ"
   },
   "outputs": [],
   "source": [
    "# Estandarizar los datos:\n",
    "medias = x_train.mean()\n",
    "stds = x_train.std()\n",
    "x_train = (x_train - medias) / stds\n",
    "x_test = (x_test - medias) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gLrOZoRRdSP"
   },
   "source": [
    "Nos quedamos sólo con los primeros 1000 ejemplos para que las ejecuciones sean más rápidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPx-zvLLRfuZ"
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI9AriBkTw70"
   },
   "source": [
    "Para no tener problemas con el tipo de dato, vamos a trabajar siempre con numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oa0v6VIgTwO4"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWBjllTLeCNn"
   },
   "source": [
    "Construye un modelo de regresión lineal para predecir la variable $t$ a partir de los atributos $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKvai5ojitYR"
   },
   "outputs": [],
   "source": [
    "d = x_train.shape[1]\n",
    "\n",
    "w = np.random.randn(1, d)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 400\n",
    "lr = 0.0001\n",
    "\n",
    "k = 1\n",
    "error = []\n",
    "for i in range(nepocas):\n",
    "    pred = np.dot(x_train, w.T) + b\n",
    "    pred = pred[:, 0]\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula el error:\n",
    "    e = 0 # TO-DO actualiza el error\n",
    "    #----------------------------------------------------------\n",
    "    error.append(e)\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula los gradientes y actualiza los parametros:\n",
    "\n",
    "    b = b  # TO-DO actualiza los parámetros\n",
    "    w = w  # TO-DO actualiza los parámetros\n",
    "    #----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_qbIXmClZix"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(nepocas), error)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFzKmKyKrnud",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
    "pred = np.dot(x_test, w.T) + b\n",
    "pred = pred[:, 0]\n",
    "\n",
    "# Grafica de y frente a t:\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(y_test, pred, 'o')\n",
    "plt.plot([0, 6], [0, 6], 'r-')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"pred\")\n",
    "plt.show()\n",
    "\n",
    "# Error:\n",
    "e = np.mean((pred-y_test)*(pred-y_test))\n",
    "print(\"Error = %f\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vfepGOjVigB"
   },
   "source": [
    "## Comprobación de California Housing con otras alternativas\n",
    "\n",
    "Vamos a comprobarlo con la regresión lineal implementada con Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S94yIfTMVYyF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iYZ33k2WcFj"
   },
   "source": [
    "### Regresión lineal siguiendo la estrategia de minimización mediante descenso por gradiente (SGD)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBUJQBNlVzFF"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAg3en1jXAZ9"
   },
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty=None).fit(x_train, y_train)\n",
    "pred = lr.predict(x_train)\n",
    "print(\"TRAIN:\", mean_squared_error(y_train, pred))\n",
    "pred = lr.predict(x_test)\n",
    "print(\"TEST:\", mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnoSmTFBHRQ_"
   },
   "outputs": [],
   "source": [
    "# La variable coef_ representa los pesos de cada atributo\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1AF3AgwHalM"
   },
   "outputs": [],
   "source": [
    "# La variable intercept_ representa el bias\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWlisovrGpt2"
   },
   "source": [
    "## Probemos la regresión lineal con regularización L1 o L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUtgGEUEGzYS"
   },
   "source": [
    "### Regularización Ridge (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zUHpod-Jvqu"
   },
   "source": [
    "La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8S0P_pwHmP7"
   },
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty='l2', alpha=0.1).fit(x_train, y_train)\n",
    "pred = lr.predict(x_train)\n",
    "print(\"TRAIN:\", mean_squared_error(y_train, pred))\n",
    "pred = lr.predict(x_test)\n",
    "print(\"TEST:\", mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2P6c3a9H6i2"
   },
   "outputs": [],
   "source": [
    "# La variable coef_ guarda todos los pesos de la regresión\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p74qNUoYIBNf"
   },
   "outputs": [],
   "source": [
    "# La variable intercept_ guarda el bias\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK0SvNYtJ6TG"
   },
   "source": [
    "Pintemos los pesos de los atributos con un diagrama de barras. Así podremos saber qué \"importancia\" le da nuestro modelo a los atributos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBpxBjrNI70z"
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(lr.coef_)), lr.coef_)\n",
    "plt.xlabel(\"Attribute ID\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBiuzHxYKUp5"
   },
   "source": [
    "Veamos ahora el error frente al factor alpha de regularización L2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfj4o4PAIB3G"
   },
   "outputs": [],
   "source": [
    "l2_values = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "errores = []\n",
    "for l2 in l2_values:\n",
    "    lr = SGDRegressor(penalty='l2', alpha=l2).fit(x_train, y_train)\n",
    "    pred = lr.predict(x_test)\n",
    "    e = mean_squared_error(y_test, pred)\n",
    "    errores.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Jd8dWBEIpG-"
   },
   "outputs": [],
   "source": [
    "plt.plot(errores, '.-')\n",
    "plt.xticks(np.arange(len(l2_values)), l2_values)\n",
    "plt.xlabel(\"Factor de regularización L2\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "# En la figura vemos que cuanto mayor penalización damos a los pesos, mayor es el MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyP5NIFRKmnZ"
   },
   "source": [
    "### Regularización Lasso (L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_ij1JflKtAz"
   },
   "source": [
    "La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njpzN168Iqxy"
   },
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty='l1', alpha=0.1).fit(x_train, y_train)\n",
    "pred = lr.predict(x_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WIsnPn8Kz-M"
   },
   "source": [
    "Pintemos los pesos de los atributos con un diagrama de barras. Así podremos saber qué \"importancia\" le da nuestro modelo a los atributos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N84Ko1oeKvTU"
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(lr.coef_)), lr.coef_)\n",
    "plt.xlabel(\"Attribute ID\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()\n",
    "\n",
    "# Con la regularización L1 (mucho más agresiva que la L2) vemos como los pesos de algunos atributos tienden a 0,\n",
    "# es decir, el modelo aprende a realizar una selección de características de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etVhLe_0K9kV"
   },
   "source": [
    "Veamos ahora el error frente al factor alpha de regularización L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_gd_pDiK1nT"
   },
   "outputs": [],
   "source": [
    "l1_values = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "errores = []\n",
    "for l1 in l1_values:\n",
    "    lr = SGDRegressor(penalty='l1', alpha=l1).fit(x_train, y_train)\n",
    "    pred = lr.predict(x_test)\n",
    "    e = mean_squared_error(y_test, pred)\n",
    "    errores.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTe4u7uaLMc5"
   },
   "outputs": [],
   "source": [
    "plt.plot(errores, '.-')\n",
    "plt.xticks(np.arange(len(l1_values)), l1_values)\n",
    "plt.xlabel(\"Factor de regularización L1\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "# En la figura vemos que cuanto mayor penalización damos a los pesos, mayor es el MSE\n",
    "# Ojo en la escala. Fíjate además como L1, comparada con L2, es más agresiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iclJhaTFRck9"
   },
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Typ17vsjRck-"
   },
   "source": [
    "La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente. Si quieres combinar ambas regularizaciones, tienes la opción de utilizar la configuración *ElasticNet*, cuya penalización se expresa de la siguiente manera:\n",
    "\n",
    "$$Penalización ElasticNet = \\alpha (\\phi L1 + (1 - \\phi) L2)$$\n",
    "\n",
    "donde $\\alpha$ es el factor de regularización (parámetro `alpha`) y $\\phi$ es el parámetro que define la contribución de las penalizaciones (parámetro `l1_ratio`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-MrafHqRck-"
   },
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty='elasticnet', alpha=0.1, l1_ratio=0.15).fit(x_train, y_train)\n",
    "pred = lr.predict(x_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Puedo intentar implementar un algoritmo de selección de atributos a partir de una regresión lineal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No olvidemos la diferencia entre selección de atributos y explicabilidad de un modelo. Hagámoslo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Y si hacemos selección de atributos según los pesos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.2, random_state=42)\n",
    "medias = x_train.mean()\n",
    "stds = x_train.std()\n",
    "x_train = (x_train - medias) / stds\n",
    "x_test = (x_test - medias) / stds\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sin regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty=None).fit(x_train.values, y_train.values)\n",
    "w = lr.coef_\n",
    "\n",
    "R = np.abs(w)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(R)), R)\n",
    "plt.xticks(range(len(R)), x.columns, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos de mayor a menor relevancia según w\n",
    "df = pd.DataFrame(R, index=x.columns, columns=[\"Relevance\"])\n",
    "df = df.sort_values(by=\"Relevance\", ascending=False)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(df)), df[\"Relevance\"])\n",
    "plt.xticks(range(len(df)), df.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orden_atributos = list(df.index)\n",
    "\n",
    "print(\" Uso de más a menos relevante\")\n",
    "errores = []\n",
    "for i in range(1, len(x.columns)+1):\n",
    "    print(orden_atributos[:i])\n",
    "    x_train_seleccion_atributos = x_train[orden_atributos[:i]].values\n",
    "    x_test_seleccion_atributos = x_test[orden_atributos[:i]].values\n",
    "    seleccion_atributos_lr = SGDRegressor(penalty=None).fit(x_train_seleccion_atributos, y_train)\n",
    "    pred = seleccion_atributos_lr.predict(x_test_seleccion_atributos)\n",
    "    errores.append(mean_squared_error(y_test, pred))\n",
    "    \n",
    "print(\" Uso de menos a más relevante\")\n",
    "errores_inv = []\n",
    "for i in range(1, len(x.columns)+1):\n",
    "    print(orden_atributos[-i:])\n",
    "    x_train_seleccion_atributos = x_train[orden_atributos[-i:]].values\n",
    "    x_test_seleccion_atributos = x_test[orden_atributos[-i:]].values\n",
    "    seleccion_atributos_lr = SGDRegressor(penalty=None).fit(x_train_seleccion_atributos, y_train)\n",
    "    pred = seleccion_atributos_lr.predict(x_test_seleccion_atributos)\n",
    "    errores_inv.append(mean_squared_error(y_test, pred))\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title(\"MSE - Selección de atributos\")\n",
    "plt.plot(errores, label=\"Mas a menos\")\n",
    "plt.plot(errores_inv, label=\"Menos a mas\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty='l1', alpha=0.05).fit(x_train.values, y_train.values)\n",
    "w = lr.coef_\n",
    "\n",
    "R = np.abs(w)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(R)), R)\n",
    "plt.xticks(range(len(R)), x.columns, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos de mayor a menor relevancia según w\n",
    "df = pd.DataFrame(R, index=x.columns, columns=[\"Relevance\"])\n",
    "df = df.sort_values(by=\"Relevance\", ascending=False)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(df)), df[\"Relevance\"])\n",
    "plt.xticks(range(len(df)), df.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orden_atributos = list(df.index)\n",
    "\n",
    "print(\" Uso de más a menos relevante\")\n",
    "errores = []\n",
    "for i in range(1, len(x.columns)+1):\n",
    "    print(orden_atributos[:i])\n",
    "    x_train_seleccion_atributos = x_train[orden_atributos[:i]].values\n",
    "    x_test_seleccion_atributos = x_test[orden_atributos[:i]].values\n",
    "    seleccion_atributos_lr = SGDRegressor(penalty=None).fit(x_train_seleccion_atributos, y_train)\n",
    "    pred = seleccion_atributos_lr.predict(x_test_seleccion_atributos)\n",
    "    errores.append(mean_squared_error(y_test, pred))\n",
    "    \n",
    "print(\" Uso de menos a más relevante\")\n",
    "errores_inv = []\n",
    "for i in range(1, len(x.columns)+1):\n",
    "    print(orden_atributos[-i:])\n",
    "    x_train_seleccion_atributos = x_train[orden_atributos[-i:]].values\n",
    "    x_test_seleccion_atributos = x_test[orden_atributos[-i:]].values\n",
    "    seleccion_atributos_lr = SGDRegressor(penalty=None).fit(x_train_seleccion_atributos, y_train)\n",
    "    pred = seleccion_atributos_lr.predict(x_test_seleccion_atributos)\n",
    "    errores_inv.append(mean_squared_error(y_test, pred))\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title(\"MSE - Selección de atributos\")\n",
    "plt.plot(errores, label=\"Mas a menos\")\n",
    "plt.plot(errores_inv, label=\"Menos a mas\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Y si intentamos explicar el modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué un dato de entrada $\\mathbf{x}_i$ ha dado un valor determinado $y_i$?\n",
    "\n",
    "La respuesta a esto se basa en: **¿Qué representa exactamente la derivada de una función??**\n",
    "\n",
    "La derivada de una función $\\frac{\\partial L(y, t)}{\\partial x_i}$ representa la tasa de cambio de la función de coste respecto a $x_i$, es decir, cómo varía la función de coste cuando varía ligeramente $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de explicabilidad específica de redes neuronales se suelen basar en la técnica del gradiente respecto a los atributos de entrada. Si quieres explicar qué tiene que pasar en la entrada para que la predicción del modelo se acerque al valor real:\n",
    "\n",
    "$$R_{\\mathbf{x}} = - |\\mathbf{x}| \\cdot \\nabla_\\mathbf{x}L(y, t)$$\n",
    "\n",
    "o si quieres explicar cómo se va a comportar el modelo:\n",
    "\n",
    "$$R_{\\mathbf{x}} = |\\mathbf{x}| \\cdot \\nabla_\\mathbf{x}(y)$$\n",
    "\n",
    "Esta técnica de explicabilidad basada en gradientes se conoce como **Gradient x Input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_x_input(x, model, t, explain_target=False):\n",
    "    y = model.predict(x)\n",
    "    w = model.coef_\n",
    "    dL_dy = (y-t)[:, None]\n",
    "    dy_dx = w[None, :]\n",
    "    dL_dx = dL_dy @ dy_dx\n",
    "\n",
    "    if explain_target:\n",
    "        R = -np.abs(x) * dL_dx\n",
    "    else:\n",
    "        R = np.abs(x) * dy_dx\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SGDRegressor(penalty='l1', alpha=0.05).fit(x_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicar el comportamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = grad_x_input(x_train.values, lr, y_train.values, explain_target=False)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a explicar el primer dato de `x_train`. Podéis probar el item que queráis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 1 # Modificar esta variable para evaluar otros ejemplos\n",
    "\n",
    "ypred_real = lr.predict(x_train.values[item][None, :])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(len(x.columns)), x_train.values[item])\n",
    "plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ypred_real, 'o', label=\"Predicción real\")\n",
    "plt.xticks([])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujemos su relevancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(x.columns)), R[item], color=\"red\")\n",
    "plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La relevancia nos indica cómo afecta cada atributo a la decisión del modelo. Si la relevancia es positiva, aumentar el atributo implica que aumenta el valor de la predicción del modelo y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(range(len(x.columns)), x_train.values[item], alpha=0.5)\n",
    "    plt.bar(range(len(x.columns)), x_train_modificado, color=\"magenta\", alpha=0.5)\n",
    "    plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(ypred_real, 'o', label=\"Predicción real\")\n",
    "    plt.plot(ypred, 'o', label=\"Predicción modificada\")\n",
    "    plt.annotate('', xy=(0, ypred[0]), xytext=(0, ypred_real[0]), arrowprops=dict(arrowstyle='->', color=\"black\"))\n",
    "    plt.xticks([])\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicabilidad(item, x_train, R, model):\n",
    "    print(\" >> La relevancia de los atributos es la siguiente:\")\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(range(len(x.columns)), R[item], color=\"red\")\n",
    "    plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo con relevancia positiva -> la predicción del modelo baja\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo con relevancia positiva -> la predicción del modelo sube\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo irrelevante -> la predicción del modelo se queda igual\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] == 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo irrelevante -> la predicción del modelo se queda igual\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] == 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo con relevancia negativa -> la predicción del modelo sube\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo con relevancia negativa -> la predicción del modelo baja\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explicabilidad(item, x_train, R, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicar qué tiene que pasar en comparación con el valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = grad_x_input(x_train.values, lr, y_train.values, explain_target=True)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0 # Modificar esta variable para evaluar otros ejemplos\n",
    "\n",
    "ypred_real = lr.predict(x_train.values[item][None, :])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(len(x.columns)), x_train.values[item])\n",
    "plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ypred_real, 'o', label=\"Predicción real\")\n",
    "plt.plot(y_train.values[item], 'o', label=\"Valor real\", color=\"magenta\")\n",
    "plt.xticks([])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujemos su relevancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(x.columns)), R[item], color=\"red\")\n",
    "plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La relevancia nos indica cómo afecta cada atributo a la decisión del modelo. Si la relevancia es positiva, aumentar el atributo implica que aumenta el valor de la predicción del modelo y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(range(len(x.columns)), x_train.values[item], alpha=0.5)\n",
    "    plt.bar(range(len(x.columns)), x_train_modificado, color=\"magenta\", alpha=0.5)\n",
    "    plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(ypred_real, 'o', label=\"Predicción real\")\n",
    "    plt.plot(ypred, 'o', label=\"Predicción modificada\")\n",
    "    plt.plot(y_train.values[item], 'o', label=\"Valor real\", color=\"magenta\")\n",
    "    plt.annotate('', xy=(0, ypred[0]), xytext=(0, ypred_real[0]), arrowprops=dict(arrowstyle='->', color=\"black\"))\n",
    "    plt.xticks([])\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicabilidad(item, x_train, R, model):\n",
    "    print(\" >> La relevancia de los atributos es la siguiente:\")\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(range(len(x.columns)), R[item], color=\"red\")\n",
    "    plt.xticks(range(len(x.columns)), x.columns, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo con relevancia positiva -> la predicción del modelo se aleja\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo con relevancia positiva -> la predicción del modelo se acerca\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo irrelevante -> la predicción del modelo se queda igual\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] == 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo irrelevante -> la predicción del modelo se queda igual\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] == 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si disminuyo un atributo con relevancia negativa -> la predicción del modelo se acerca\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] - 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "    \n",
    "    print(\" >> Si aumento un atributo con relevancia negativa -> la predicción del modelo se aleja\")\n",
    "    x_train_modificado = x_train.iloc[item].copy()\n",
    "    i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "    x_train_modificado.iloc[i_columna] = x_train_modificado.iloc[i_columna] + 0.2\n",
    "    ypred = model.predict(x_train_modificado.values[None, :])\n",
    "    ypred_real = model.predict(x_train.iloc[item].values[None, :])\n",
    "    dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explicabilidad(item, x_train, R, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
