{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CZBOKsOZnf"
      },
      "source": [
        "# <font color=\"#CA3532\">Introducción a TensorFlow</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9GMdFSVORkA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u26frSE2QcdL"
      },
      "source": [
        "### <font color=\"#CA3532\">Conceptos básicos</font>\n",
        "\n",
        "**<font color=\"#CA3532\">Tensores:</font>** Un tensor es un array de datos con un número arbitrario de dimensiones. El *rango* del tensor es el número de dimensiones del mismo.\n",
        "\n",
        "- Un escalar es un tensor de rango 0\n",
        "- Un vector es un tensor de rango 1\n",
        "- Una matriz es un tensor de rango 2\n",
        "\n",
        "Algunos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Tv5gd0QMng"
      },
      "outputs": [],
      "source": [
        "# Escalar, tensor de rango 0:\n",
        "t0 = tf.constant(3.)\n",
        "print(t0)\n",
        "print(t0.numpy())\n",
        "print(tf.shape(t0).numpy())\n",
        "print(tf.rank(t0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3XfELMLQ3_i"
      },
      "outputs": [],
      "source": [
        "# Vector, tensor de rango 1:\n",
        "t1 = tf.constant([1., 2., 3.])\n",
        "print(t1)\n",
        "print(t1.numpy())\n",
        "print(tf.shape(t1).numpy())\n",
        "print(tf.rank(t1).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ke5UqH3Q8uX"
      },
      "outputs": [],
      "source": [
        "# Matriz, tensor de rango 2:\n",
        "t2 = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(t2)\n",
        "print(t2.numpy())\n",
        "print(tf.shape(t2).numpy())\n",
        "print(tf.rank(t2).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SdOQ2kzRAa2"
      },
      "outputs": [],
      "source": [
        "# Tensor de rango 3:\n",
        "t3 = tf.constant([[[1., 2., 3.]], [[7., 8., 9.]]])\n",
        "print(t3)\n",
        "print(t3.numpy())\n",
        "print(tf.shape(t3).numpy())\n",
        "print(tf.rank(t3).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYh4km95Ta66"
      },
      "source": [
        "### <font color=\"#CA3532\">Grafo computacional</font>\n",
        "\n",
        "Cada vez que realizamos una operación con tensores, TensorFlow construye un grafo:\n",
        "\n",
        "- Los tensores constantes (o variables) son las entradas al grafo.\n",
        "\n",
        "- La información se propaga por el grafo de izquierda a derecha (forward) y en cada nodo se realiza una operación.\n",
        "\n",
        "Ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xllDk9OkRFGG"
      },
      "outputs": [],
      "source": [
        "# Dos tensores constantes que entran en el grafo:\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "# Suma (puedes usar tf.add o el operador +):\n",
        "c1 = tf.add(a, b)\n",
        "c2 = a + b\n",
        "print(c1)\n",
        "print(c2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_OfXBoGxBkr"
      },
      "source": [
        "Compatibilidad con Numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqKV2CYeTyYw"
      },
      "outputs": [],
      "source": [
        "z = a + np.ones((3, 3))\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_qipMkZhqFk"
      },
      "source": [
        "### <font color=\"#CA3532\">Backward pass</font>\n",
        "\n",
        "El grafo computacional también permite hacer una pasada hacia atrás (de derecha a izquierda) para calcular los gradientes. Para poder hacerlo es necesario:\n",
        "\n",
        "1. Que el grafo contenga variables (tensores creados con tf.Variable)\n",
        "2. Que el grafo se cree en el contexto de un *Gradient Tape*\n",
        "\n",
        "Ejemplo: vamos a crear un grafo para la operación $3x^{2}$, donde $x$ es una variable. La variable tendrá inicialmente el valor $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWAI0sg4D5Av"
      },
      "outputs": [],
      "source": [
        "# Creamos constante y variable:\n",
        "a = tf.constant(3.0)\n",
        "x = tf.Variable(1.0)\n",
        "\n",
        "# Definimos el grafo dentro de un Gradient Tape:\n",
        "with tf.GradientTape() as tape:\n",
        "  y = a*x*x\n",
        "\n",
        "# Imprimimos el valor de y:\n",
        "print(\"y =\", y)\n",
        "print(\"y =\", y.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMg0kIjbjGy7"
      },
      "source": [
        "Para calcular el gradiente usamos el Gradient Tape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQsAONPui1LD"
      },
      "outputs": [],
      "source": [
        "dy_dx = tape.gradient(y, x)\n",
        "print(\"dy_dx =\", dy_dx)\n",
        "print(\"dy_dx =\", dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbxjoEljVep"
      },
      "source": [
        "Es mejor poner todo el código dentro de una función para poder reutilizarlo con más facilidad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eYHB0jNjdBM"
      },
      "outputs": [],
      "source": [
        "def ax2(a, x):\n",
        "  x = tf.Variable(x)\n",
        "\n",
        "  # Definicion del grafo:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = a*x**2 # Notese que a se interpreta como tensor constante\n",
        "\n",
        "  # Calculo del gradiente:\n",
        "  dy_dx = tape.gradient(y, x)\n",
        "\n",
        "  return y, dy_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12dnHerfkwnd"
      },
      "source": [
        "Probemos algunos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uucc22Jfjc9w"
      },
      "outputs": [],
      "source": [
        "y, dy_dx = ax2(3.0, 1.0)\n",
        "print(\"y =\", y)\n",
        "print(\"dy_dx =\", dy_dx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct-3kxHdkzS2"
      },
      "outputs": [],
      "source": [
        "y, dy_dx = ax2(3.0, 3.0)\n",
        "print(\"y =\", y)\n",
        "print(\"dy_dx =\", dy_dx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSml8UhbF_07"
      },
      "outputs": [],
      "source": [
        "x = np.arange(-5, 5, 0.1)\n",
        "print(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czHHGBE7GqB8"
      },
      "outputs": [],
      "source": [
        "y, dy_dx = ax2(3.0, x)\n",
        "print(y.shape)\n",
        "print(dy_dx.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, dy_dx)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7cMX56_k4k4"
      },
      "source": [
        "Puedes comprobar que tanto el cálculo de la función como su derivada son correctos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1gUonlxlIVM"
      },
      "source": [
        "**Ejercicio:** Construye una función que calcule el valor y el gradiente con respecto a $x$ de la expresión $y = \\sin{x}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cbSqYHYljTN"
      },
      "outputs": [],
      "source": [
        "def seno(x):\n",
        "  x = tf.Variable(x)\n",
        "\n",
        "  # Definicion del grafo:\n",
        "  with tf.GradientTape() as tape:\n",
        "    # TO-DO (tf.sin)\n",
        "\n",
        "  # Calculo del gradiente:\n",
        "  # TO-DO\n",
        "\n",
        "  return y, dy_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fGIy3j2lzMy"
      },
      "source": [
        "Usa la función anterior para dibujar una gráfica de la función seno y su derivada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPTYO7qWljRC"
      },
      "outputs": [],
      "source": [
        "x = np.arange(0, 2*np.pi, 0.01)\n",
        "y, dy_dx = seno(x) # Observa que puedo pasar un array a la funcion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB8R2u3tmQNv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x, y, label=\"y\")\n",
        "plt.plot(x, dy_dx, label=\"dy_dx\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOInLMfhnHa2"
      },
      "source": [
        "**Ejercicio:** Construye una función que calcule el valor y el gradiente con respecto a $x$ de la expresión $y = \\sigma{(x)}$. Recuerda que la función sigmoide es esta:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXP0f9IDnZI0"
      },
      "outputs": [],
      "source": [
        "def sigmoide(x):\n",
        "  x = tf.Variable(x)\n",
        "\n",
        "  # Definicion del grafo:\n",
        "  with tf.GradientTape() as tape:\n",
        "    # TO-DO (tf.sigmoid o bien tf.exp)\n",
        "\n",
        "  # Calculo del gradiente:\n",
        "  # TO-DO\n",
        "\n",
        "  return y, dy_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWUPVY5snkSW"
      },
      "source": [
        "Usa la función anterior para dibujar una gráfica de la función sigmoide y su derivada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EnOTstPnmHN"
      },
      "outputs": [],
      "source": [
        "x = np.arange(-10., 10., 0.01)\n",
        "y, dy_dx = sigmoide(x) # Observa que puedo pasar un array a la funcion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubnrG4R4nu02"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(x, y)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(x, dy_dx)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"dy_dx\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ee1zztdjk9"
      },
      "source": [
        "**Ejercicio:** Construye una función que reciba otra función ``f(x)``, donde ``x`` es un array de Numpy, y devuelva una función que evalúe su derivada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gx5NQKFdyfQ"
      },
      "outputs": [],
      "source": [
        "def differentiate(f):\n",
        "  # TO-DO..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpi9J3z0jD7K"
      },
      "source": [
        "Podríamos usar esta función del siguiente modo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XulHHhHJenRG"
      },
      "outputs": [],
      "source": [
        "def my_sine(x):\n",
        "  return tf.sin(x)\n",
        "dsin = differentiate(my_sine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyu_wVcrfAuG"
      },
      "outputs": [],
      "source": [
        "x = np.arange(0, 2*np.pi, 0.01)\n",
        "y = my_sine(x)\n",
        "dy = dsin(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGH3lMEbfEum"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x, y, label=\"y\")\n",
        "plt.plot(x, dy, label=\"dy_dx\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbjKGf_Booaj"
      },
      "source": [
        "### <font color=\"#CA3532\">Descenso por gradiente</font>\n",
        "\n",
        "Por supuesto, podemos utilizar los gradientes calculados por la diferenciación automática para optimizar una determinada función. A continuación tienes un ejemplo.\n",
        "\n",
        "Queremos encontrar el mínimo de la función $y = (x - x_{0})^{2}$. No es difícil hallar analíticamente que el mínimo se produce cuando $x = x_{0}$. Pero vamos a hacerlo numéricamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djOF1ywDWllb"
      },
      "outputs": [],
      "source": [
        "# Inicializamos la x al azar:\n",
        "x = tf.Variable(tf.random.normal((1,)))\n",
        "print(\"Valor inicial x = \", x.numpy())\n",
        "\n",
        "# Factor de aprendizaje:\n",
        "learning_rate = 1.e-2\n",
        "\n",
        "# Lista para almacenar los valores que va tomando x:\n",
        "history = []\n",
        "\n",
        "# Valor objetivo, x_0:\n",
        "x_0 = 4\n",
        "\n",
        "# Numero de iteraciones:\n",
        "niters = 500\n",
        "\n",
        "# Bucle de optimizacion:\n",
        "for i in range(niters):\n",
        "  # Definicion del grafo:\n",
        "  # TO-DO\n",
        "\n",
        "  # Calculo del gradiente:\n",
        "  # TO-DO\n",
        "\n",
        "  # Actualizacion de x:\n",
        "  new_x = x - learning_rate*grad\n",
        "  x.assign(new_x) # Ojo, usamos el metodo assign para asignar un valor a una variable\n",
        "\n",
        "  # Actualizamos la lista de valores de x:\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# Grafica que muestra la evolucion de x:\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500], [x_0, x_0])\n",
        "plt.grid(True)\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxfeUrXnZhww"
      },
      "source": [
        "### <font color=\"#CA3532\">Regresión lineal</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OPddKslYsiK"
      },
      "outputs": [],
      "source": [
        "# Parametros:\n",
        "d = 5 # Dimension del problema\n",
        "w = np.random.randn(d, 1)\n",
        "b = 1.0\n",
        "xmin = 0.0\n",
        "xmax = 10.0\n",
        "noise = 1.0\n",
        "n = 1000\n",
        "\n",
        "# Datos del problema generados al azar:\n",
        "x = xmin + np.random.rand(d, n)*(xmax - xmin)\n",
        "t0 = np.dot(w.T,x) + b\n",
        "t = t0 + np.random.randn(n)*noise\n",
        "tmin = np.min(t)\n",
        "tmax = np.max(t)\n",
        "\n",
        "# Distribucion de las dos primeras variables:\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "plt.plot(x[0,:], x[1,:], 'o')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.subplot(122)\n",
        "\n",
        "# Grafica de t frente a t0:\n",
        "plt.plot(t0[0], t[0], 'o')\n",
        "plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"t0\")\n",
        "plt.ylabel(\"t\")\n",
        "plt.show()\n",
        "\n",
        "# Error esperado:\n",
        "e = np.sum((t-t0)*(t-t0))\n",
        "print(\"Error esperado = %f\" % e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c20hYxlocVJW"
      },
      "outputs": [],
      "source": [
        "x = tf.constant(x)\n",
        "t = tf.constant(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yDAZBSVb2lW"
      },
      "outputs": [],
      "source": [
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncf5CsCQb6Lm"
      },
      "outputs": [],
      "source": [
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rRnjOgzaM5f"
      },
      "source": [
        "Modelo en tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNPn3aNuZnQQ"
      },
      "outputs": [],
      "source": [
        "w = tf.Variable(tf.random.normal(shape=[d, 1], dtype=tf.dtypes.float64))\n",
        "b = tf.Variable(tf.random.normal(shape=[1], dtype=tf.dtypes.float64))\n",
        "\n",
        "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
        "# tf.matmul y @ son equivalentes:\n",
        "#y = tf.matmul(tf.transpose(w), x) + b\n",
        "y = tf.transpose(w)@x + b\n",
        "\n",
        "# Grafica de y frente a t:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t[0], y[0], 'o')\n",
        "plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Error:\n",
        "e = np.sum((y-t)*(y-t))\n",
        "print(\"Error = %f\" % e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AexfCI9rdtKi"
      },
      "outputs": [],
      "source": [
        "nepocas = 64\n",
        "eta = 0.005\n",
        "\n",
        "error = []\n",
        "for i in range(nepocas):\n",
        "  # Definicion del grafo:\n",
        "  # Ojo, persistent = True porque vamos a calcular gradientes respecto a mas de\n",
        "  # una variable (vamos a llamar varias veces a tape.gradient)\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Calculo de la salida y:\n",
        "    # TO-DO - tf.matmul y @ son equivalentes:\n",
        "\n",
        "    # Calculo de la funcion de coste:\n",
        "    loss = tf.reduce_mean((y - t)**2)\n",
        "\n",
        "  error.append(loss.numpy())\n",
        "\n",
        "  # Calculo de los gradientes:\n",
        "  # TO-DO\n",
        "\n",
        "  # Actualizacion de los parametros:\n",
        "  # TO-DO (ojo assign)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SlQuynOhFiD"
      },
      "outputs": [],
      "source": [
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v3Gr6oShGJY"
      },
      "outputs": [],
      "source": [
        "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
        "y = tf.matmul(tf.transpose(w), x) + b\n",
        "\n",
        "# Grafica de y frente a t:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t[0], y[0], 'o')\n",
        "plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Error:\n",
        "e = np.sum((y-t)*(y-t))\n",
        "print(\"Error = %f\" % e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V5qrgfCoHgg"
      },
      "source": [
        "Con un tape no persistente y llamando sólo una vez a tape.gradient:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PJB9T0EoPTz"
      },
      "outputs": [],
      "source": [
        "w = tf.Variable(tf.random.normal(shape=[d, 1], dtype=tf.dtypes.float64))\n",
        "b = tf.Variable(tf.random.normal(shape=[1], dtype=tf.dtypes.float64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_U5TcKWoGMN"
      },
      "outputs": [],
      "source": [
        "nepocas = 64\n",
        "eta = 0.005\n",
        "\n",
        "error = []\n",
        "for i in range(nepocas):\n",
        "  # Definicion del grafo:\n",
        "  # Ojo, persistent = True porque vamos a calcular gradientes respecto a mas de\n",
        "  # una variable (vamos a llamar varias veces a tape.gradient)\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Calculo de la salida y:\n",
        "    # TO-DO - tf.matmul y @ son equivalentes:\n",
        "\n",
        "    # Calculo de la funcion de coste:\n",
        "    loss = tf.reduce_mean((y - t)**2)\n",
        "\n",
        "  error.append(loss.numpy())\n",
        "\n",
        "  # Calculo de los gradientes:\n",
        "  # TO-DO\n",
        "\n",
        "  # Actualizacion de los parametros:\n",
        "  # TO-DO (ojo assign)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyj5y4k7onVj"
      },
      "outputs": [],
      "source": [
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCdbCpg3rqsE"
      },
      "source": [
        "### <font color=\"#CA3532\">Ejercicios</font>\n",
        "\n",
        "<b><font color=\"#CA3532\">Ejercicios de regresión - California Housing</font></b>\n",
        "\n",
        "1. Regresión lineal.\n",
        "\n",
        "  1.1. Construir una clase *LinearRegressionModel* en tensorflow mediante una RN sin capas ocultas.\n",
        "\n",
        "  1.2. Aplicar el modelo al problema *California Housing*.\n",
        "  \n",
        "  1.3. Comparar los resultados con los del modelo implementado en Numpy (ver notebook ``02_lin_reg_exercise.ipynb``).\n",
        "\n",
        "2. Red neuronal para regresión.\n",
        "\n",
        "  2.1. Construir una clase *NeuralNetworkRegressionModel* en tensorflow con un número arbitrario de capas ocultas.\n",
        "\n",
        "  2.2. Aplicar el modelo al problema *California Housing*.\n",
        "\n",
        "  2.3. Comparar los resultados con los del modelo implementado en Numpy (ver notebook ``04_red_simple_para_clase.ipynb``).\n",
        "\n",
        "<b><font color=\"#CA3532\">Ejercicios de clasificación - Breast Cancer Wisconsin</font></b>\n",
        "\n",
        "3. Regresión logística.\n",
        "\n",
        "  3.1. Construir una clase *LogisticRegressionModel* en tensorflow mediante una RN sin capas ocultas.\n",
        "\n",
        "  3.2. Aplicar el modelo al problema *Breast Cancer Wisconsin*.\n",
        "\n",
        "  3.3. Comparar los resultados con los del modelo implementado en Numpy (ver notebook ``03_log_reg_exercise.ipynb``).\n",
        "\n",
        "4. Red neuronal para clasificación.\n",
        "\n",
        "  4.1. Construir una clase *NeuralNetworkModel* en tensorflow con un número arbitrario de capas ocultas y solamente 1 neurona en la capa de salida con activación sigmoid.\n",
        "\n",
        "  4.2. Aplicar el modelo al problema *Breast Cancer Wisconsin*.\n",
        "\n",
        "  4.3. Comparar los resultados con los del modelo implementado en Numpy (ver notebook ``04_red_simple_para_clase.ipynb``)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYBwCbO_7y4-"
      },
      "source": [
        "## <font color=\"#CA3532\">Ejercicios de regresión - California Housing</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgfkFCxOc4jR"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhPShJZmJkhf"
      },
      "outputs": [],
      "source": [
        "x, t = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "print(x.shape)\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnfotf1cSfRG"
      },
      "outputs": [],
      "source": [
        "x = x.values[:1000]\n",
        "t = t.values[:1000]\n",
        "t = t[:, None]\n",
        "\n",
        "print(x.shape)\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRuxlf1Mf0fJ"
      },
      "outputs": [],
      "source": [
        "# Particion entrenamiento-test:\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=12)\n",
        "\n",
        "# Estandarizacion:\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5IDKwMVJ3aP"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(t_train.shape)\n",
        "print(x_test.shape)\n",
        "print(t_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EFIvdtC7rqc"
      },
      "source": [
        "### <font color=\"#CA3532\">Ejercicio 1: Regresión lineal con California Housing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD7y5GOQ7_kk"
      },
      "source": [
        "1.1. Construir una clase LinearRegressionModel en tensorflow mediante una RN sin capas ocultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4X_R0um79MM"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionModel:\n",
        "\n",
        "  def __init__(self, d0=2):\n",
        "    self.W = tf.Variable(tf.random.normal(shape=[d0, 1], dtype=tf.dtypes.float64))\n",
        "    self.b = tf.Variable(tf.random.normal(shape=[1], dtype=tf.dtypes.float64))\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    x must be a (n,d0) array\n",
        "    returns a (n,1) array with the predictions for each of the n patterns\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula la y para regresión lineal\n",
        "    y = 0\n",
        "    return y\n",
        "\n",
        "  def mse(self, x, t):\n",
        "    \"\"\"\n",
        "    computes the MSE between the model predictions and the targets\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula el error cuadrático medio utilizando la función tf.reduce_mean\n",
        "    mse = 0\n",
        "    return mse\n",
        "\n",
        "  def fit(self, x, t, eta, num_epochs):\n",
        "    \"\"\"\n",
        "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
        "    num_epochs epochs\n",
        "    \"\"\"\n",
        "    mse_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "      with tf.GradientTape() as tape:\n",
        "        # TO-DO: Define el cálculo forward del error dentro de GradientTape para que\n",
        "        #        se guarden las derivadas\n",
        "\n",
        "      mse_history.append(mse)\n",
        "\n",
        "      [db, dW] = tape.gradient(mse, [self.b, self.W])\n",
        "      self.b.assign(self.b - eta*db)\n",
        "      self.W.assign(self.W - eta*dW)\n",
        "\n",
        "    return mse_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVAOWfxIpUNL"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# TO-DO: Construcción y entrenamiento del modelo\n",
        "\n",
        "# Construccion del modelo\n",
        "\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GrmN84hrMMZ"
      },
      "source": [
        "Evaluar en training y test después del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSZD3wM_pUHN"
      },
      "outputs": [],
      "source": [
        "print(\"MSE training:\", model.mse(x_train, t_train).numpy())\n",
        "print(\"MSE test:\", model.mse(x_test, t_test).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2b4w9fe-9Ng"
      },
      "source": [
        "Si quisieramos evaluar el test en cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy7P8KNN_OzJ"
      },
      "outputs": [],
      "source": [
        "# Construccion del modelo\n",
        "n, d = x.shape\n",
        "\n",
        "model = LinearRegressionModel(d)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "eta = 0.01\n",
        "epochs = 200\n",
        "\n",
        "error_training = []\n",
        "error_test = []\n",
        "for e in range(epochs):\n",
        "  error = model.fit(x_train, t_train, eta, 1)\n",
        "  error_training.append(error)\n",
        "  error = model.mse(x_test, t_test)\n",
        "  error_test.append(error)\n",
        "plt.plot(error_training, label=\"Train\")\n",
        "plt.plot(error_test, label=\"Test\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp8B9bTAAALi"
      },
      "source": [
        "Visualizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3lnPiCGADPk"
      },
      "outputs": [],
      "source": [
        "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
        "y = model.predict(x_test)\n",
        "\n",
        "# Grafica de y frente a t:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t_test, y, 'o')\n",
        "plt.plot([0, 6], [0, 6], 'r-')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Error:\n",
        "e = model.mse(x_test, t_test)\n",
        "print(\"Error = %f\" % e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEqV6d1JAl2c"
      },
      "source": [
        "### <font color=\"#CA3532\">Ejercicio 2: Red neuronal para regresión - California Housing</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkFCp2MnAwFL"
      },
      "source": [
        "2.1. Construir una clase NeuralNetworkRegressionModel en tensorflow con un número arbitrario de capas ocultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g6G5QSZA3dl"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkRegressionModel:\n",
        "\n",
        "  def __init__(self, layers_size=[2]):\n",
        "    self.W = [tf.Variable(tf.random.normal(shape=[a, b], dtype=tf.dtypes.float64)) for a, b in zip(layers_size[:-1], layers_size[1:])]\n",
        "    self.b = [tf.Variable(tf.random.normal(shape=[1, b], dtype=tf.dtypes.float64)) for b in layers_size[1:]]\n",
        "\n",
        "    self.W.append(tf.Variable(tf.random.normal(shape=[layers_size[-1], 1], dtype=tf.dtypes.float64)))\n",
        "    self.b.append(tf.Variable(tf.random.normal(shape=[1, 1], dtype=tf.dtypes.float64)))\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    x must be a (n,d0) array\n",
        "    returns a (n,1) array with the predictions for each of the n patterns\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula la y para la red neuronal de regresión\n",
        "    y = 0\n",
        "    return y\n",
        "\n",
        "  def mse(self, x, t):\n",
        "    \"\"\"\n",
        "    computes the MSE between the model predictions and the targets\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula el error cuadrático medio utilizando la función tf.reduce_mean\n",
        "    mse = 0\n",
        "    return mse\n",
        "\n",
        "  def fit(self, x, t, eta, num_epochs):\n",
        "    \"\"\"\n",
        "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
        "    num_epochs epochs\n",
        "    \"\"\"\n",
        "    mse_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "      # OJO: Es necesario persistent=True porque vamos a querer obtener los gradientes\n",
        "      #      de W y bias por capa.\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        # TO-DO: Define el cálculo forward del error dentro de GradientTape para que\n",
        "        #        se guarden las derivadas\n",
        "\n",
        "      mse_history.append(mse)\n",
        "\n",
        "      # Aquí calculamos los gradientes por capa, por eso necesitamos persistent=True\n",
        "      # en el gradientTape\n",
        "      for b, W in zip(self.b, self.W):\n",
        "        [db, dW] = tape.gradient(mse, [b, W])\n",
        "        b.assign(b - eta*db)\n",
        "        W.assign(W - eta*dW)\n",
        "\n",
        "    return mse_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1S3IOvSBZ0j"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# TO-DO: Construcción y entrenamiento del modelo\n",
        "\n",
        "# Construccion del modelo\n",
        "\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKDTsJ6-sPnT"
      },
      "source": [
        "Evaluar en training y test después del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo-Aqo9mB62-"
      },
      "outputs": [],
      "source": [
        "print(\"MSE training:\", model.mse(x_train, t_train).numpy())\n",
        "print(\"MSE test:\", model.mse(x_test, t_test).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRTq77zNCBog"
      },
      "source": [
        "Si quisieramos evaluar el test en cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A03fBKafsPnU"
      },
      "outputs": [],
      "source": [
        "# Construccion del modelo\n",
        "n, input_dimension = x.shape\n",
        "hidden_layers_d = [input_dimension, 10, 5]\n",
        "\n",
        "model = NeuralNetworkRegressionModel(hidden_layers_d)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "eta = 0.03\n",
        "epochs = 500\n",
        "\n",
        "error_training = []\n",
        "error_test = []\n",
        "for e in range(epochs):\n",
        "  error = model.fit(x_train, t_train, eta, 1)\n",
        "  error_training.append(error)\n",
        "  error = model.mse(x_test, t_test)\n",
        "  error_test.append(error)\n",
        "plt.plot(error_training, label=\"Train\")\n",
        "plt.plot(error_test, label=\"Test\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipm9B8B7CupW"
      },
      "source": [
        "Visualizar los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbXbER7asPnU"
      },
      "outputs": [],
      "source": [
        "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
        "y = model.predict(x_test)\n",
        "\n",
        "# Grafica de y frente a t:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(t_test, y, 'o')\n",
        "plt.plot([0, 6], [0, 6], 'r-')\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Error:\n",
        "e = model.mse(x_test, t_test)\n",
        "print(\"Error = %f\" % e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey9NfuS6gmYq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ5RIN8BDX4A"
      },
      "source": [
        "## <font color=\"#CA3532\">Ejercicios de clasificación - Breast Cancer Wisconsin</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24BNQv-eDhgX"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAhIDM8gDmxB"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "x = data.data\n",
        "t = data.target[:, None]\n",
        "print(x.shape)\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC890fuUEAZ3"
      },
      "outputs": [],
      "source": [
        "# Particion entrenamiento-test:\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=12)\n",
        "\n",
        "# Estandarizacion:\n",
        "means = x_train.mean(axis=0)\n",
        "stds = x_train.std(axis=0)\n",
        "x_train = (x_train - means) / stds\n",
        "x_test = (x_test - means) / stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNK2SMGIEFxb"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(t_train.shape)\n",
        "print(x_test.shape)\n",
        "print(t_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZU_mi7UC7P-"
      },
      "source": [
        "### <font color=\"#CA3532\">Ejercicio 3: Regresión logística con Breast Cancer Wisconsin</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qllSNJ0jDE1q"
      },
      "source": [
        "3.1. Construir una clase LogisticRegressionModel en tensorflow mediante una RN sin capas ocultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqSQIByRDFwU"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionModel:\n",
        "\n",
        "  def __init__(self, d0=2):\n",
        "    self.W = tf.Variable(tf.random.normal(shape=[d0, 1], dtype=tf.dtypes.float64))\n",
        "    self.b = tf.Variable(tf.random.normal(shape=[1], dtype=tf.dtypes.float64))\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    x must be a (n,d0) array\n",
        "    returns a (n,1) array with the predictions for each of the n patterns\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula la y para la red neuronal de clasificación\n",
        "    y = 0\n",
        "    return y\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    \"\"\"\n",
        "    computes the cross-entropy between the model predictions and the targets\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula el cross-entropy loss. Puedes calcularlo a mano con tf.reduce_mean\n",
        "    #        o utilizar alguna función definida en tensorflow para calcular crossentropy\n",
        "    #        OJO: que no esté en la librería tf.keras\n",
        "    loss = 0\n",
        "    return loss\n",
        "\n",
        "  def fit(self, x, t, eta, num_epochs):\n",
        "    \"\"\"\n",
        "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
        "    num_epochs epochs\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "      with tf.GradientTape() as tape:\n",
        "        # TO-DO: Define el cálculo forward del loss dentro de GradientTape para que\n",
        "        #        se guarden las derivadas\n",
        "\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      [db, dW] = tape.gradient(loss, [self.b, self.W])\n",
        "      self.b.assign(self.b - eta*db)\n",
        "      self.W.assign(self.W - eta*dW)\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x).numpy()\n",
        "    pred = y > 0.5\n",
        "    return np.mean(pred == t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED6pKvJVELVU"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# TO-DO: Construcción y entrenamiento del modelo\n",
        "\n",
        "# Construccion del modelo\n",
        "\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bazNAO5VFnlZ"
      },
      "source": [
        "Evaluar en training y test después del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omcty4qwFqXY"
      },
      "outputs": [],
      "source": [
        "print(\"Loss training:\", model.loss(x_train, t_train).numpy())\n",
        "print(\"Acc training:\", model.accuracy(x_train, t_train))\n",
        "print(\"Loss test:\", model.loss(x_test, t_test).numpy())\n",
        "print(\"Acc test:\", model.accuracy(x_test, t_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_V1fFpnFw56"
      },
      "source": [
        "Si queremos evaluar en test en cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO7fqxf_FzCn"
      },
      "outputs": [],
      "source": [
        "# Construccion del modelo\n",
        "n, d = x.shape\n",
        "\n",
        "model = LogisticRegressionModel(d)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "eta = 0.1\n",
        "epochs = 500\n",
        "\n",
        "loss_training = []\n",
        "acc_training = []\n",
        "loss_test = []\n",
        "acc_test = []\n",
        "for e in range(epochs):\n",
        "  loss = model.fit(x_train, t_train, eta, 1)\n",
        "  loss_training.append(loss)\n",
        "  acc = model.accuracy(x_train, t_train)\n",
        "  acc_training.append(acc)\n",
        "\n",
        "  loss = model.loss(x_test, t_test)\n",
        "  loss_test.append(loss)\n",
        "  acc = model.accuracy(x_test, t_test)\n",
        "  acc_test.append(acc)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), loss_training, label=\"train\")\n",
        "plt.plot(range(epochs), loss_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(epochs), acc_training, label=\"train\")\n",
        "plt.plot(range(epochs), acc_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDaJZKlpIp6M"
      },
      "outputs": [],
      "source": [
        "print(\"Loss training:\", model.loss(x_train, t_train))\n",
        "print(\"Acc training:\", model.accuracy(x_train, t_train))\n",
        "print(\"Loss test:\", model.loss(x_test, t_test))\n",
        "print(\"Acc test:\", model.accuracy(x_test, t_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxJ7j_JoHL7J"
      },
      "source": [
        "### <font color=\"#CA3532\">Ejercicio 4: Red neuronal para clasificación - Breast Cancer Wisconsin</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q05yKuxHP_6"
      },
      "source": [
        "4.1. Construir una clase NeuralNetworkModel en tensorflow con un número arbitrario de capas ocultas y solamente 1 neurona en la capa de salida con activación sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0i31TfSHQ7g"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkModel:\n",
        "\n",
        "  def __init__(self, layers_size=[2]):\n",
        "    self.W = [tf.Variable(tf.random.normal(shape=[a, b], dtype=tf.dtypes.float64)) for a, b in zip(layers_size[:-1], layers_size[1:])]\n",
        "    self.b = [tf.Variable(tf.random.normal(shape=[1, b], dtype=tf.dtypes.float64)) for b in layers_size[1:]]\n",
        "\n",
        "    self.W.append(tf.Variable(tf.random.normal(shape=[layers_size[-1], 1], dtype=tf.dtypes.float64)))\n",
        "    self.b.append(tf.Variable(tf.random.normal(shape=[1, 1], dtype=tf.dtypes.float64)))\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    x must be a (n,d0) array\n",
        "    returns a (n,1) array with the predictions for each of the n patterns\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula la y para la red neuronal de clasificación\n",
        "    y = 0\n",
        "    return y\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    \"\"\"\n",
        "    computes the MSE between the model predictions and the targets\n",
        "    \"\"\"\n",
        "    # TO-DO: Calcula el cross-entropy loss. Puedes calcularlo a mano con tf.reduce_mean\n",
        "    #        o utilizar alguna función definida en tensorflow para calcular crossentropy\n",
        "    #        OJO: que no esté en la librería tf.keras\n",
        "    loss = 0\n",
        "    return loss\n",
        "\n",
        "  def fit(self, x, t, eta, num_epochs):\n",
        "    \"\"\"\n",
        "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
        "    num_epochs epochs\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "      # OJO: Es necesario persistent=True porque vamos a querer obtener los gradientes\n",
        "      #      de W y bias por capa.\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        # TO-DO: Define el cálculo forward del loss dentro de GradientTape para que\n",
        "        #        se guarden las derivadas\n",
        "\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      # Aquí calculamos los gradientes por capa, por eso necesitamos persistent=True\n",
        "      # en el gradientTape\n",
        "      for b, W in zip(self.b, self.W):\n",
        "        [db, dW] = tape.gradient(loss, [b, W])\n",
        "        b.assign(b - eta*db)\n",
        "        W.assign(W - eta*dW)\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x).numpy()\n",
        "    pred = y > 0.5\n",
        "    return np.mean(pred == t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVRl7fWJdAm"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# TO-DO: Construcción y entrenamiento del modelo\n",
        "\n",
        "# Construccion del modelo\n",
        "\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "plt.plot(error)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTLQeAXJxYK"
      },
      "source": [
        "Evaluación en training y test después del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPLHUtQsJuz7"
      },
      "outputs": [],
      "source": [
        "print(\"Loss training:\", model.loss(x_train, t_train))\n",
        "print(\"Acc training:\", model.accuracy(x_train, t_train))\n",
        "print(\"Loss test:\", model.loss(x_test, t_test))\n",
        "print(\"Acc test:\", model.accuracy(x_test, t_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xZgJySWJZn3"
      },
      "source": [
        "Si queremos evaluar test en cada época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGlVwL8WJ42R"
      },
      "outputs": [],
      "source": [
        "# Construccion del modelo\n",
        "n, input_dimension = x.shape\n",
        "hidden_layers_d = [5]\n",
        "\n",
        "model = NeuralNetworkModel([input_dimension] + hidden_layers_d)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "eta = 0.2\n",
        "epochs = 2000\n",
        "\n",
        "loss_training = []\n",
        "acc_training = []\n",
        "loss_test = []\n",
        "acc_test = []\n",
        "for e in range(epochs):\n",
        "  loss = model.fit(x_train, t_train, eta, 1)\n",
        "  loss_training.append(loss)\n",
        "  acc = model.accuracy(x_train, t_train)\n",
        "  acc_training.append(acc)\n",
        "\n",
        "  loss = model.loss(x_test, t_test)\n",
        "  loss_test.append(loss)\n",
        "  acc = model.accuracy(x_test, t_test)\n",
        "  acc_test.append(acc)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), loss_training, label=\"train\")\n",
        "plt.plot(range(epochs), loss_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(epochs), acc_training, label=\"train\")\n",
        "plt.plot(range(epochs), acc_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlkZw8QTKEG-"
      },
      "outputs": [],
      "source": [
        "print(\"Loss training:\", model.loss(x_train, t_train).numpy())\n",
        "print(\"Acc training:\", model.accuracy(x_train, t_train))\n",
        "print(\"Loss test:\", model.loss(x_test, t_test).numpy())\n",
        "print(\"Acc test:\", model.accuracy(x_test, t_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7mBYfZb-R8"
      },
      "source": [
        "# <font color=\"#CA3532\">Entrenamiento con mini-batches</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVSK5CN6uJQ"
      },
      "source": [
        "Hasta ahora hemos visto que el entrenamiento de un modelo se hace por épocas, pasando en cada época todos los datos disponibles por el modelo para calcular los gradientes. Sin embargo, este método de entrenamiento tiene una gran desventaja: **es necesario procesar todo el dataset de entrenamiento para realizar una única modificación de los parámetros de la red**.\n",
        "\n",
        "En problemas pequeños, como los que hemos visto, este inconveniente no supone ningún problema. Sin embargo, en problemas reales, donde pueden llegarse a procesar millones de datos, es inviable seguir con esta estrategia.\n",
        "\n",
        "<b><font color=\"#CA3532\">Solución:</font> Entrenar mediante lotes (batches)</b>\n",
        "\n",
        "El entrenamiento con mini-batches consiste en actualizar los pesos después de que el modelo haya procesado un número determinado de datos. Esto implica que **en una época haya más de una actualización de los parámetros de la red**. El tamaño del batch (*batch_size*) será entonces un hiperparámetro más para ajustar.\n",
        "\n",
        "Vamos a modificar la clase *NeuralNetworkModel* anterior para añadir un método *train_on_batch*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ameeZhb59SgF"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkModel:\n",
        "\n",
        "  def __init__(self, layers_size=[2]):\n",
        "    self.W = [tf.Variable(tf.random.normal(shape=[a, b], dtype=tf.dtypes.float64)) for a, b in zip(layers_size[:-1], layers_size[1:])]\n",
        "    self.b = [tf.Variable(tf.random.normal(shape=[1, b], dtype=tf.dtypes.float64)) for b in layers_size[1:]]\n",
        "\n",
        "    self.W.append(tf.Variable(tf.random.normal(shape=[layers_size[-1], 1], dtype=tf.dtypes.float64)))\n",
        "    self.b.append(tf.Variable(tf.random.normal(shape=[1, 1], dtype=tf.dtypes.float64)))\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    x must be a (n,d0) array\n",
        "    returns a (n,1) array with the predictions for each of the n patterns\n",
        "    \"\"\"\n",
        "    y = x\n",
        "    for w, b in zip(self.W, self.b):\n",
        "      z = tf.matmul(y, w) + b\n",
        "      y = tf.sigmoid(z)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    \"\"\"\n",
        "    computes the cross-entropy loss between the model predictions and the targets\n",
        "    \"\"\"\n",
        "    y = self.predict(x)\n",
        "    loss = tf.reduce_mean(-t*tf.math.log(y) - (1.-t)*tf.math.log(1.-y), axis=0)\n",
        "    return loss\n",
        "\n",
        "  def train_on_batch(self, x, t, eta):\n",
        "    \"\"\"\n",
        "    Makes a parameter update using one single batch of data.\n",
        "    \"\"\"\n",
        "    # En esta función train_on_batch, asumimos que x y t son un subconjunto de\n",
        "    # los datos de train completos. Cómo dividir los datos en batches se lo dejamos\n",
        "    # al usuario cuando llame a la función\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      loss = self.loss(x, t)\n",
        "\n",
        "    for b, W in zip(self.b, self.W):\n",
        "      [db, dW] = tape.gradient(loss, [b, W])\n",
        "      b.assign(b - eta*db)\n",
        "      W.assign(W - eta*dW)\n",
        "\n",
        "  def fit(self, x, t, eta, num_epochs):\n",
        "    \"\"\"\n",
        "    Fits the model parameters with data (x, t) using a learning rate eta and\n",
        "    num_epochs epochs\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "        loss = self.loss(x, t)\n",
        "\n",
        "      loss_history.append(loss.numpy().ravel()[0])\n",
        "\n",
        "      for b, W in zip(self.b, self.W):\n",
        "        [db, dW] = tape.gradient(loss, [b, W])\n",
        "        b.assign(b - eta*db)\n",
        "        W.assign(W - eta*dW)\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x).numpy()\n",
        "    pred = y > 0.5\n",
        "    return np.mean(pred == t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1V_mrCYrZdi"
      },
      "source": [
        "Entrenamiento con un único batch con todos los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sJ9O8KWi38B"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "n, d = x_train.shape\n",
        "eta = 0.1\n",
        "num_epochs = 500\n",
        "\n",
        "nlrm = NeuralNetworkModel([d, 32])\n",
        "\n",
        "loss_train = nlrm.loss(x_train, t_train)\n",
        "print(\"Loss (train) antes de entrenar:\", loss_train.numpy()[0])\n",
        "\n",
        "# Entrenamiento:\n",
        "loss = nlrm.fit(x_train, t_train, eta, num_epochs)\n",
        "\n",
        "loss_train = nlrm.loss(x_train, t_train)\n",
        "print(\"Loss (train) despues de entrenar:\", loss_train.numpy()[0])\n",
        "loss_test = nlrm.loss(x_test, t_test)\n",
        "print(\"Loss (test) despues de entrenar:\", loss_test.numpy()[0])\n",
        "\n",
        "acc_train = nlrm.accuracy(x_train, t_train)\n",
        "print(\"Accuracy (train) después de entrenar:\", acc_train)\n",
        "acc_test = nlrm.accuracy(x_test, t_test)\n",
        "print(\"Accuracy (test) después de entrenar:\", acc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3yq28-ki38B"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(num_epochs), loss)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Cross Entropy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYm52k9Ai38B"
      },
      "source": [
        "Entrenamiento por batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bs1L7__i38B"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "n, d = x_train.shape\n",
        "eta = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "batch_size = 10\n",
        "num_batches = np.ceil(n/batch_size).astype(int)\n",
        "\n",
        "nlrm = NeuralNetworkModel([d, 32])\n",
        "\n",
        "# Entrenamiento, es necesario implementar el bucle aqui:\n",
        "loss_train = []\n",
        "loss_test = []\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "for epoch in range(num_epochs):\n",
        "  # Permutacion aleatoria para desordenar los datos:\n",
        "  ix = np.random.permutation(n)\n",
        "\n",
        "  # Genero los batches y ajusto pesos con cada batch:\n",
        "  for j in range(num_batches):\n",
        "    imin = j*batch_size\n",
        "    imax = np.minimum(n, (j+1)*batch_size)\n",
        "    x_batch = x_train[ix[imin:imax]]\n",
        "    t_batch = t_train[ix[imin:imax]]\n",
        "\n",
        "    nlrm.train_on_batch(x_batch, t_batch, eta)\n",
        "\n",
        "  # Al finalizar cada epoca calculo el error en training y test:\n",
        "  loss_train.append(nlrm.loss(x_train, t_train).numpy())\n",
        "  loss_test.append(nlrm.loss(x_test, t_test).numpy())\n",
        "  acc_train.append(nlrm.accuracy(x_train, t_train))\n",
        "  acc_test.append(nlrm.accuracy(x_test, t_test))\n",
        "\n",
        "print(\"Loss (train) despues de entrenar:\", loss_train[-1][0])\n",
        "print(\"Loss (test) despues de entrenar:\", loss_test[-1][0])\n",
        "print(\"Accuracy (train) después de entrenar:\", acc_train[-1])\n",
        "print(\"Accuracy (test) después de entrenar:\", acc_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq2ApcFoi38C"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(num_epochs), loss_train, label=\"train\")\n",
        "plt.plot(range(num_epochs), loss_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Cross-entropy loss\")\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(num_epochs), acc_train, label=\"train\")\n",
        "plt.plot(range(num_epochs), acc_test, label=\"test\")\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY1-D1k-rxiF"
      },
      "source": [
        "**Discutir los resultados**\n",
        "\n",
        "¿Has comparado el número de épocas del entrenamiento de los dos casos?\n",
        "\n",
        "¿Cuál tarda más en dar una vuelta completa de los datos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DexQ1bTOLdcA"
      },
      "source": [
        "## <font color=\"#CA3532\">Preguntas del último día</font>\n",
        "\n",
        "¿Cómo podríamos construir un modelo multiclase?\n",
        "\n",
        "1. Con regresión logística, ¿qué podemos hacer?\n",
        "\n",
        "2. Con redes neuronales, ¿qué podemos hacer?\n",
        "\n",
        "Lo vemos en el notebook ``04_02_problema_multiclase.ipynb``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FTX-tginwzw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}