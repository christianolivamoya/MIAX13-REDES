{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoFZV1kBZkAi"
   },
   "source": [
    "<font color=\"#CA3532\"><h1 align=\"left\">Inteligencia Artificial Aplicada a la Bolsa (MIAX-13)</h1></font>\n",
    "<font color=\"#5b5a59\"><h2 align=\"left\">Extensión del modelo de regresión logística a una dimensión arbitraria</h2></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74sT4NIPYtiy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_jtvCMsLRLZ"
   },
   "source": [
    "Definición de la función sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7pJwv0uLT6b"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uyab5aRGOChN"
   },
   "source": [
    "Cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zu_s4EgOCM4"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    loss = np.mean(-t*np.log(y) - (1.-t)*np.log(1.-y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFGdcxIKTTe6"
   },
   "source": [
    "Cross-entropy loss, from logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUp5ukIxTXHg"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(z, t):\n",
    "    a = np.log(1. + np.exp(-z))\n",
    "    loss = np.mean(a + z - t*z)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCSIOQeic-3J"
   },
   "source": [
    "Precisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjWeLlWrdBxE"
   },
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    pred = y > 0.5\n",
    "    return np.mean(pred == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkQ5ae7LZ-zc"
   },
   "source": [
    "Generación de los datos del problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "3__RYeMyZyEg",
    "outputId": "3a84f167-083d-47ca-fc12-cd1cd6c5c215"
   },
   "outputs": [],
   "source": [
    "# Parametros:\n",
    "d = 5 # Dimension del problema\n",
    "w = np.random.randn(d, 1)\n",
    "b = 0.0\n",
    "xmin = -10.0\n",
    "xmax = 10.0\n",
    "noise = 1.0\n",
    "n = 1000\n",
    "\n",
    "# Datos del problema generados al azar:\n",
    "x = xmin + np.random.rand(d, n)*(xmax - xmin)\n",
    "z = np.dot(w.T, x) + b\n",
    "zmin = np.min(z)\n",
    "zmax = np.max(z)\n",
    "t0 = sigmoid(z)\n",
    "t = 1*(t0 > np.random.rand(n))\n",
    "\n",
    "# Distribucion de las dos primeras variables:\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(x[0,:], x[1,:], 'o')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.subplot(122)\n",
    "\n",
    "# Grafica de t frente a z:\n",
    "zrange = np.arange(zmin, zmax, 0.1)\n",
    "plt.plot(z[0], t[0], 'o', label='data points')\n",
    "plt.plot(zrange, sigmoid(zrange), 'r-', label='true prob $P(t = 1 | x)$')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Coste esperado:\n",
    "loss = cross_entropy(t0, t)\n",
    "print(\"Cross-entropy esperada = %f\" % loss)\n",
    "loss = cross_entropy_from_logits(z, t)\n",
    "print(\"Cross-entropy esperada (logits) = %f\" % loss)\n",
    "\n",
    "# Accuracy:\n",
    "acc = accuracy(t0, t)\n",
    "print(\"Accuracy esperada = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nax9A91aaNc0"
   },
   "source": [
    "Forma de los vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGYdnMLDZ4nG",
    "outputId": "e22453c6-3d8c-49e3-a1b7-6f1f96d39cfc"
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRgATzqpad5m"
   },
   "source": [
    "Modelo de regresión logística con los parámetros inicializados al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "YrbCLsvPaTRI",
    "outputId": "fc8c0839-725a-4955-81e0-148833cc6d2f"
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n",
    "z = np.dot(w.T, x) + b\n",
    "zmin = np.min(z)\n",
    "zmax = np.max(z)\n",
    "y = sigmoid(z)\n",
    "\n",
    "# Grafica de y frente a z:\n",
    "zrange = np.arange(zmin, zmax, 0.1)\n",
    "plt.plot(z[0], t[0], 'o', label='data points')\n",
    "plt.plot(zrange, sigmoid(zrange), 'r-', label='model')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Coste:\n",
    "loss = cross_entropy_from_logits(z, t)\n",
    "print(\"Cross-entropy = %f\" % loss)\n",
    "\n",
    "# Accuracy:\n",
    "acc = accuracy(y, t)\n",
    "print(\"Accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad_HnpDMas0N"
   },
   "source": [
    "Entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iAWYGrCDah2I",
    "outputId": "14ddb468-3166-4a08-d350-ed8825016fcd"
   },
   "outputs": [],
   "source": [
    "nepocas = 64\n",
    "eta = 0.0001\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "k = 1\n",
    "loss = []\n",
    "acc = []\n",
    "for i in range(nepocas):\n",
    "    z = np.dot(w.T, x) + b\n",
    "    zmin = np.min(z)\n",
    "    zmax = np.max(z)\n",
    "    y = sigmoid(z)\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula el coste cross-entropy y la precision (acc),\n",
    "    # y añadelos a las listas loss y acc:\n",
    "    e = 0\n",
    "    a = 0\n",
    "    loss.append(e)\n",
    "    acc.append(a)\n",
    "    #----------------------------------------------------------\n",
    "\n",
    "    if i%4 == 0:\n",
    "        zrange = np.arange(zmin, zmax, 0.1)\n",
    "        plt.subplot(4, 4, k)\n",
    "        plt.plot(z[0], t[0], 'o')\n",
    "        plt.plot(zrange, sigmoid(zrange), 'r-')\n",
    "        plt.grid(True)\n",
    "        plt.title(\"epoca = %d, loss = %.2f\" % (i, loss[-1]))\n",
    "        k += 1\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # TO-DO: Calcula los gradientes y actualiza los parametros:\n",
    "    b = b\n",
    "    w = w\n",
    "    #----------------------------------------------------------\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93E3pGM5a51W"
   },
   "source": [
    "Cross-entropy y accuracy frente a número de épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "nzR5BJdWaubX",
    "outputId": "cb176736-5f47-4e00-a29c-5b1d60909dae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(nepocas), loss)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGCKEb6eJBto",
    "outputId": "2118e009-2ba9-4cd2-f43e-c503f13ac86e"
   },
   "outputs": [],
   "source": [
    "loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbIIEAxdJL41",
    "outputId": "b7108451-6901-4803-a8e2-8354d4db97aa"
   },
   "outputs": [],
   "source": [
    "acc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uwP6KLQc2qn"
   },
   "source": [
    "# Breast Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgfkFCxOc4jR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhI-RDZdXN4"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3id3P7PIdjzG",
    "outputId": "6ccde6b5-39f7-4e87-abf4-188e34fd67f9"
   },
   "outputs": [],
   "source": [
    "x = data.data\n",
    "t = data.target[:, None]\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCSsIMMra87k"
   },
   "source": [
    "La clase $t$ toma dos posibles valores (0 y 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpcfXbjEa4YF",
    "outputId": "c7288757-c278-47e4-e0ae-44f987381e30"
   },
   "outputs": [],
   "source": [
    "print(np.unique(t))\n",
    "print(np.sum(t == 0))\n",
    "print(np.sum(t == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWBjllTLeCNn"
   },
   "source": [
    "Vamos a construir un modelo de regresión logística para predecir la variable $t$ a partir de los atributos $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG4a0V9PJ1OR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY2Ead3VJ6Hp"
   },
   "outputs": [],
   "source": [
    "# Particion entrenamiento-test:\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=42)\n",
    "x_train = x_train\n",
    "x_test = x_test\n",
    "t_train = t_train\n",
    "t_test = t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tdu_leuAKunb",
    "outputId": "9d8fbdde-63fd-420f-e1ce-3f994b16bc4b"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2txzl8gJebV"
   },
   "outputs": [],
   "source": [
    "# Estandarizar los datos\n",
    "media = x_train.mean(axis=0, keepdims=True)\n",
    "std = x_train.std(axis=0, keepdims=True)\n",
    "\n",
    "x_train = (x_train - media) / std\n",
    "x_test = (x_test - media) / std # Se estandariza con la media y std de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CzmvlobMV4B"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo:\n",
    "\n",
    "d = x_train.shape[1]\n",
    "\n",
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 400\n",
    "eta = 0.0001\n",
    "\n",
    "k = 1\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(nepocas):\n",
    "    # TO-DO Calcula cross-entropy y accuracy en training\n",
    "    e_train = 0\n",
    "    a_train = 0\n",
    "\n",
    "    loss_train.append(e_train)\n",
    "    acc_train.append(a_train)\n",
    "\n",
    "    # TO-DO Calcula cross-entropy y accuracy en test\n",
    "    e_test = 0\n",
    "    a_test = 0\n",
    "\n",
    "    loss_test.append(e_test)\n",
    "    acc_test.append(a_test)\n",
    "\n",
    "    # TO-DO Calcula los gradientes y actualiza los parametros\n",
    "    b = b\n",
    "    w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "0S6N0RLDNv2Z",
    "outputId": "b2bc4e58-6a84-4eb7-ddef-1789f60d56b7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(nepocas), loss_train, label=\"train\")\n",
    "plt.plot(range(nepocas), loss_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc_train, label=\"train\")\n",
    "plt.plot(range(nepocas), acc_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyOMLLlor0Th",
    "outputId": "ba57dc72-518e-41b3-f2e0-c2eee5a0b3c1"
   },
   "outputs": [],
   "source": [
    "acc_test[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udx9Cmn2pnl1"
   },
   "source": [
    "## Reducción de dimensionalidad con PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJMsgnwUp1xM"
   },
   "source": [
    "Principal Component Analysis (PCA) es un método estadístico que permite simplificar la complejidad reduciendo la dimensión de un problema mediante un método lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyAhni-PpXD8"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWx-WM34rJFk"
   },
   "source": [
    "Primero podemos analizar la información que se pierde al transformar el conjunto de datos a un espacio de menor dimensión. Para ello creamos un PCA y analizamos su ratio de varianza explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "rCNBNME8quMa",
    "outputId": "6ca93ec8-6597-480a-c00a-4924009d24f9"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "plt.plot(1 - pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRoX7vCMqd1X"
   },
   "source": [
    "Se ajusta la transformación indicando el número de dimensiones a la que queremos reducir. Usamos el parámetro *n_components*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqMRJvEfq5L-",
    "outputId": "7a25e471-1bd2-4383-f406-845728d96962"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(x_train)\n",
    "print(1 - pca.explained_variance_ratio_[-1]) # Con 10 dimensiones tenemos un 98.78% de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc98JrRWsTBt"
   },
   "source": [
    "Transformamos los datos de training y test a las 10 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cGQwk-9sGXH"
   },
   "outputs": [],
   "source": [
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cttD1UTysafc"
   },
   "source": [
    "Entrenamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcjwMQUKsXSB"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo:\n",
    "\n",
    "d = x_train_pca.shape[1]\n",
    "\n",
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 400\n",
    "eta = 0.0001\n",
    "\n",
    "k = 1\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(nepocas):\n",
    "    # TO-DO Calcula cross-entropy y accuracy en training\n",
    "    e_train = 0\n",
    "    a_train = 0\n",
    "\n",
    "    loss_train.append(e_train)\n",
    "    acc_train.append(a_train)\n",
    "\n",
    "    # TO-DO Calcula cross-entropy y accuracy en test\n",
    "    e_test = 0\n",
    "    a_test = 0\n",
    "\n",
    "    loss_test.append(e_test)\n",
    "    acc_test.append(a_test)\n",
    "\n",
    "    # TO-DO Calcula los gradientes y actualiza los parametros\n",
    "    b = b\n",
    "    w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "DU-7yKKTshY5",
    "outputId": "c2364101-dd43-40ae-bd63-ff9bb428d332"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(nepocas), loss_train, label=\"train\")\n",
    "plt.plot(range(nepocas), loss_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc_train, label=\"train\")\n",
    "plt.plot(range(nepocas), acc_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63ZrkCcWsjCG",
    "outputId": "58d18e80-b4a1-4647-8899-f0bba4229366"
   },
   "outputs": [],
   "source": [
    "acc_test[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv5U63rLs3dF"
   },
   "source": [
    "## Extraer la frontera de decisión de un modelo en 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVUWlmbLtDew"
   },
   "source": [
    "En este caso, vamos a reducir mediante PCA la dimensión de los datos a 2D para dibujar la frontera de decisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18NP43Xlsl7O",
    "outputId": "8a17b671-353b-4d75-c153-b527450b1ab4"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train)\n",
    "print(1 - pca.explained_variance_ratio_[-1]) # Con 2 dimensiones tenemos un 80.15% de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zZxxesotXr4"
   },
   "source": [
    "Transformamos los datos de training y test a 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usSZgUHJtQwt"
   },
   "outputs": [],
   "source": [
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FefpobztZ7M"
   },
   "source": [
    "Entrenamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1I988w8tVSX"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo:\n",
    "\n",
    "d = x_train_pca.shape[1]\n",
    "\n",
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 400\n",
    "eta = 0.0001\n",
    "\n",
    "k = 1\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(nepocas):\n",
    "    # TO-DO Calcula cross-entropy y accuracy en training\n",
    "    e_train = 0\n",
    "    a_train = 0\n",
    "\n",
    "    loss_train.append(e_train)\n",
    "    acc_train.append(a_train)\n",
    "\n",
    "    # TO-DO Calcula cross-entropy y accuracy en test\n",
    "    e_test = 0\n",
    "    a_test = 0\n",
    "\n",
    "    loss_test.append(e_test)\n",
    "    acc_test.append(a_test)\n",
    "\n",
    "    # TO-DO Calcula los gradientes y actualiza los parametros\n",
    "    b = b\n",
    "    w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "YDBVCKa0tbs9",
    "outputId": "2a40d459-4f7b-4a0e-dc8d-9a099a226c59"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(nepocas), loss_train, label=\"train\")\n",
    "plt.plot(range(nepocas), loss_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc_train, label=\"train\")\n",
    "plt.plot(range(nepocas), acc_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7GV5MeOtjgX",
    "outputId": "f9d5ddc0-f069-49ac-948a-5a4f804e45a9"
   },
   "outputs": [],
   "source": [
    "acc_test[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaQFGxsFGwqf"
   },
   "source": [
    "Para dibujar la frontera de decisión en 2D, se calcula una malla con la función *meshgrid* de numpy. Esta función de da los pares de puntos a imprimir en un plot con la función *contourf*. Además, hay que calcular el valor de cada punto, por lo que hay que procesarlos con el modelo ya entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0ZhcGDOuTm2",
    "outputId": "ec2a8c61-6a6b-4a27-84ae-178b4cf264b4"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_train_pca[:, 0].min()-1, x_train_pca[:, 0].max()+1, 0.1),\n",
    "                     np.arange(x_train_pca[:, 1].min()-1, x_train_pca[:, 1].max()+1, 0.1))\n",
    "xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuKQDTxau0Y2",
    "outputId": "ecd636ea-0b1d-404d-e783-2b397ef958a6"
   },
   "outputs": [],
   "source": [
    "z = sigmoid(xy @ w + b)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "NqHrd7LLtnjC",
    "outputId": "00fc8019-1560-408f-c765-5b4bba3b7b02"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.contourf(xx, yy, z[:, 0].reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "plt.plot(x_train_pca[t_train.ravel()==0, 0], x_train_pca[t_train.ravel()==0, 1], 'o', label=\"Clase 0\", color='blue')\n",
    "plt.plot(x_train_pca[t_train.ravel()==1, 0], x_train_pca[t_train.ravel()==1, 1], 'o', label=\"Clase 1\", color='red')\n",
    "\n",
    "plt.title(\"Probabilidad de Clase 1\")\n",
    "plt.xlabel(\"PCA-1\")\n",
    "plt.ylabel(\"PCA-2\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJkSszyFwoL8"
   },
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1Kkx8kOwx8F"
   },
   "source": [
    "Este problema sencillo que ya conocéis consiste en clasificar plantas de la especie Iris en tres subespecies: virgínica, setosa y versicolor. Los atributos que describen cada planta son las dimensiones (longitud y anchura) del pétalo y sépalo.\n",
    "\n",
    "En este enlace tenéis una descripción de los datos:\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYcO1L_7wADW"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr44mvxnxPF8"
   },
   "source": [
    "Preparación de los datos considerando exclusivamente las dos últimas dimensiones del problema (longitud y anchura del pétalo). Esto nos permite visualizar el modelo en 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJQOa2dCxXj-"
   },
   "outputs": [],
   "source": [
    "x = iris.data[:, -2:]\n",
    "t = iris.target\n",
    "[n, d] = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ddAfEuxZzb"
   },
   "source": [
    "Los datos del problema están ordenados. Para evitar sesgos conviene desordenarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn_-GuRKxdux",
    "outputId": "c17c7970-4f1a-4208-f71f-454dbaf419aa"
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(n)\n",
    "x = x[p, :]\n",
    "t = t[p]\n",
    "t = t[:, None]\n",
    "print(x.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47taBapzyFD0"
   },
   "source": [
    "Ahora imprimimos los datos en 2D. Se puede ver que los dos atributos seleccionados distinguen relativamente bien las tres clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "sepv-9YjxCXX",
    "outputId": "43eabd29-482f-4e61-b02c-12cf8da93256"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(x[t.ravel()==0, 0], x[t.ravel()==0, 1], 'o', label=iris.target_names[0], color='#993300')\n",
    "plt.plot(x[t.ravel()==1, 0], x[t.ravel()==1, 1], 'o', label=iris.target_names[1], color='#009933')\n",
    "plt.plot(x[t.ravel()==2, 0], x[t.ravel()==2, 1], 'o', label=iris.target_names[2], color='#000099')\n",
    "\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISQkboYrzYbU"
   },
   "source": [
    "Separamos el dataset en training-test para preparar los datos de los ejercicios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dACe2ERPzOaF"
   },
   "outputs": [],
   "source": [
    "# Particion entrenamiento-test:\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=42)\n",
    "x_train = x_train\n",
    "x_test = x_test\n",
    "t_train = t_train\n",
    "t_test = t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFNTEX1Izh1p",
    "outputId": "d227f34d-152b-4e94-d64e-bd28bcdfd51a"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c1i48SQyZfh"
   },
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "1.1. Crear un modelo de regresión logística que aprenda a diferenciar la subespecie setosa de las demás subespecies del dataset.\n",
    "\n",
    "1.2. Visualizar la frontera de decisión del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho--R5ah0FSg",
    "outputId": "427f0803-afbf-44c8-ee1c-f35855d5133c"
   },
   "outputs": [],
   "source": [
    "clase_setosa = np.where(iris.target_names == 'setosa')[0][0]\n",
    "clase_setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMpcS0b4zyyy"
   },
   "outputs": [],
   "source": [
    "x_train_e1 = x_train\n",
    "x_test_e1 = x_test\n",
    "\n",
    "# Tenemos que quedarnos con la clase setosa como clase target (1) y el resto reetiquetarlas como no-target (0).\n",
    "t_train_e1 = (t_train == clase_setosa) * 1\n",
    "t_test_e1 = (t_test == clase_setosa) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "NXa-4R-Z0sIQ",
    "outputId": "a1324410-bdc9-473b-b460-1543e8d69c7a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Training\", size=18)\n",
    "plt.plot(x_train_e1[(t_train_e1==1)[:, 0], 0], x_train_e1[(t_train_e1==1)[:, 0], 1], 'o', label=iris.target_names[0], color='red')\n",
    "plt.plot(x_train_e1[(t_train_e1==0)[:, 0], 0], x_train_e1[(t_train_e1==0)[:, 0], 1], 'o', label=\"others\", color='blue')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\", size=18)\n",
    "plt.plot(x_test_e1[(t_test_e1==1)[:, 0], 0], x_test_e1[(t_test_e1==1)[:, 0], 1], 'o', label=iris.target_names[0], color='red')\n",
    "plt.plot(x_test_e1[(t_test_e1==0)[:, 0], 0], x_test_e1[(t_test_e1==0)[:, 0], 1], 'o', label=\"others\", color='blue')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLrp1opSzn2I"
   },
   "outputs": [],
   "source": [
    "# TO-DO Entrenamiento del modelo:\n",
    "\n",
    "d = x_train_e1.shape[1]\n",
    "\n",
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 500\n",
    "eta = 0.001\n",
    "\n",
    "k = 1\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(nepocas):\n",
    "    # TO-DO Calcula cross-entropy y accuracy en training\n",
    "    e_train = 0\n",
    "    a_train = 0\n",
    "\n",
    "    loss_train.append(e_train)\n",
    "    acc_train.append(a_train)\n",
    "\n",
    "    # TO-DO Calcula cross-entropy y accuracy en test\n",
    "    e_test = 0\n",
    "    a_test = 0\n",
    "\n",
    "    loss_test.append(e_test)\n",
    "    acc_test.append(a_test)\n",
    "\n",
    "    # TO-DO Calcula los gradientes y actualiza los parametros\n",
    "    b = b\n",
    "    w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "pscHtg1wzwHD",
    "outputId": "40749006-81b0-483e-f270-4e068d2eea03"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(nepocas), loss_train, label=\"train\")\n",
    "plt.plot(range(nepocas), loss_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc_train, label=\"train\")\n",
    "plt.plot(range(nepocas), acc_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f67fnmKK1OMu",
    "outputId": "900f9292-ce76-4bbb-d9d7-09616af30707"
   },
   "outputs": [],
   "source": [
    "acc_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2nIIP-q1PY5",
    "outputId": "c3d35ce0-a1f7-4e53-9d14-598b59b274c5"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_train_e1[:, 0].min()-1, x_train_e1[:, 0].max()+1, 0.1),\n",
    "                     np.arange(x_train_e1[:, 1].min()-1, x_train_e1[:, 1].max()+1, 0.1))\n",
    "xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSdRnwea1VHo",
    "outputId": "6c19ceb9-5495-426a-ccd5-204a67c0016e"
   },
   "outputs": [],
   "source": [
    "z = sigmoid(xy @ w + b)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "IiX5zTS41X_0",
    "outputId": "1d5a7316-3851-46ad-c8a0-ac2ae0110f73"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.contourf(xx, yy, z[:, 0].reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "plt.plot(x_test_e1[t_test_e1.ravel()==0, 0], x_test_e1[t_test_e1.ravel()==0, 1], 'o', label=\"others\", color='blue')\n",
    "plt.plot(x_test_e1[t_test_e1.ravel()!=0, 0], x_test_e1[t_test_e1.ravel()!=0, 1], 'o', label='setosa', color='red')\n",
    "\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L25Z9YGxyoCp"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "2.1. Crear un modelo de regresión logística (sin modificar los dos atributos de entrada) que aprenda a diferenciar la subespecie versicolor de las demás subespecies del dataset.\n",
    "\n",
    "2.2. Visualizar la frontera de decisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nkgPO_Z4zav",
    "outputId": "8cc746e7-e4e9-468c-f2d5-55293f14c91e"
   },
   "outputs": [],
   "source": [
    "clase_versicolor = np.where(iris.target_names == 'versicolor')[0][0]\n",
    "clase_versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKHTwczY42cj"
   },
   "outputs": [],
   "source": [
    "x_train_e2 = x_train\n",
    "x_test_e2 = x_test\n",
    "\n",
    "# Tenemos que quedarnos con la clase versicolor como clase target (1) y el resto reetiquetarlas como no-target (0).\n",
    "t_train_e2 = (t_train == clase_versicolor) * 1\n",
    "t_test_e2 = (t_test == clase_versicolor) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "Ims8JV0u4-Mq",
    "outputId": "69ce1502-b170-4dc4-db6e-18a70d3458d9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Training\", size=18)\n",
    "plt.plot(x_train_e2[(t_train_e2==1)[:, 0], 0], x_train_e2[(t_train_e2==1)[:, 0], 1], 'o', label=iris.target_names[0], color='red')\n",
    "plt.plot(x_train_e2[(t_train_e2==0)[:, 0], 0], x_train_e2[(t_train_e2==0)[:, 0], 1], 'o', label=\"others\", color='blue')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\", size=18)\n",
    "plt.plot(x_test_e2[(t_test_e2==1)[:, 0], 0], x_test_e2[(t_test_e2==1)[:, 0], 1], 'o', label=iris.target_names[0], color='red')\n",
    "plt.plot(x_test_e2[(t_test_e2==0)[:, 0], 0], x_test_e2[(t_test_e2==0)[:, 0], 1], 'o', label=\"others\", color='blue')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgVcADRM5J_2"
   },
   "outputs": [],
   "source": [
    "# TO-DO Entrenamiento del modelo:\n",
    "\n",
    "d = x_train_e2.shape[1]\n",
    "\n",
    "w = np.random.randn(d, 1)\n",
    "b = np.random.randn()\n",
    "\n",
    "nepocas = 500\n",
    "eta = 0.001\n",
    "\n",
    "k = 1\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(nepocas):\n",
    "    # TO-DO Calcula cross-entropy y accuracy en training\n",
    "    e_train = 0\n",
    "    a_train = 0\n",
    "\n",
    "    loss_train.append(e_train)\n",
    "    acc_train.append(a_train)\n",
    "\n",
    "    # TO-DO Calcula cross-entropy y accuracy en test\n",
    "    e_test = 0\n",
    "    a_test = 0\n",
    "\n",
    "    loss_test.append(e_test)\n",
    "    acc_test.append(a_test)\n",
    "\n",
    "    # TO-DO Calcula los gradientes y actualiza los parametros\n",
    "    b = b\n",
    "    w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "KO08zqSQ5Q2J",
    "outputId": "d174b37d-c6cb-4e9f-ef82-167842308766"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(nepocas), loss_train, label=\"train\")\n",
    "plt.plot(range(nepocas), loss_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(nepocas), acc_train, label=\"train\")\n",
    "plt.plot(range(nepocas), acc_test, label=\"test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKqyy2LB5UWU",
    "outputId": "78890c4e-b9a7-4dc3-d5ba-3727f3e8dac6"
   },
   "outputs": [],
   "source": [
    "acc_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twVhVbWx5SJc",
    "outputId": "1dd05632-917c-45f2-b670-99c462d90cde"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_train_e2[:, 0].min()-1, x_train_e2[:, 0].max()+1, 0.1),\n",
    "                     np.arange(x_train_e2[:, 1].min()-1, x_train_e2[:, 1].max()+1, 0.1))\n",
    "xy = np.concatenate([xx.reshape([1, -1]), yy.reshape([1, -1])],axis=0).T\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3w4hQk_p5Yew",
    "outputId": "517244e9-1280-4ca3-f2d6-f591b9893cd3"
   },
   "outputs": [],
   "source": [
    "z = sigmoid(xy @ w + b)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "DJNKngMt5aqB",
    "outputId": "fe5a44f6-981b-4227-cf02-beab2ff4c694"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.contourf(xx, yy, z[:, 0].reshape(xx.shape), 100, cmap=\"bwr\", alpha=0.4, vmin=0.0, vmax=1.0)\n",
    "\n",
    "plt.plot(x_test_e2[t_test_e2.ravel()==0, 0], x_test_e2[t_test_e2.ravel()==0, 1], 'o', label=\"others\", color='blue')\n",
    "plt.plot(x_test_e2[t_test_e2.ravel()!=0, 0], x_test_e2[t_test_e2.ravel()!=0, 1], 'o', label=\"versicolor\", color='red')\n",
    "\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.grid(True)\n",
    "plt.legend(loc=2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQl65SHj9vTj"
   },
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF-7b8lP_VoD"
   },
   "source": [
    "Vamos a construir un modelo de regresión logística multinomial y explicarlo utilizando SKlearn. Este modelo se puede como una **red neuronal sin capas ocultas** como en el siguiente grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3rSRs-8_oo_"
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def draw_network(inputs, outputs):\n",
    "  # Crear un grafo dirigido\n",
    "  dot = Digraph(comment='Ejemplo de Grafo Dirigido')\n",
    "  dot.attr(rankdir='LR')\n",
    "\n",
    "  # Agregar nodos\n",
    "  for i in range(inputs):\n",
    "    dot.node(\"i\"+str(i+1), \"\", shape=\"point\", style='invis')\n",
    "    dot.node('x'+str(i+1), 'x'+str(i+1))\n",
    "    dot.edge(\"i\"+str(i+1), \"x\"+str(i+1))\n",
    "\n",
    "  for i in range(outputs):\n",
    "    dot.node('y'+str(i+1), 'y'+str(i+1))\n",
    "    dot.node(\"o\"+str(i+1), \"\", shape=\"point\", style='invis')\n",
    "    dot.edge(\"y\"+str(i+1), \"o\"+str(i+1))\n",
    "\n",
    "  for i in range(inputs):\n",
    "    for j in range(outputs):\n",
    "      dot.edge('x'+str(i+1), 'y'+str(j+1))\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "y5r3kkx0ALpz",
    "outputId": "8e7b7180-872b-4780-9ccb-acb765a3a1c7"
   },
   "outputs": [],
   "source": [
    "draw_network(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teWDH_d_BSNm"
   },
   "source": [
    "La salida de este modelo está definida como:\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{x} \\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "$$y_i = softmax_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "donde $\\mathbf{x}$ es el vector de atributos de entrada, $\\mathbf{W}$ es la matriz de pesos que conecta la entrada con cada neurona de salida y $\\mathbf{b}$ es el vector de *bias* de cada salida. Así, el vector $\\mathbf{y}$ representa la probabilidad asignada del modelo a cada una de las clases.\n",
    "\n",
    "SKlearn implementa esta regresión logística multinomial (red neuronal sin capas ocultas) por defecto. Vamos a utilizarla para resolver el problema.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKngxC4iDL22"
   },
   "outputs": [],
   "source": [
    "# Definición de softmax\n",
    "softmax = lambda z: (np.exp(z) / np.exp(z).sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izlaY6GEP4DG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNAZkUsw-c7F",
    "outputId": "d7420b15-3396-4466-b7e0-f0aee1cf2b17"
   },
   "outputs": [],
   "source": [
    "# Cargo de nuevo el dataset\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "t = iris.target\n",
    "\n",
    "np.random.seed(42)\n",
    "p = np.random.permutation(n)\n",
    "x = x[p, :]\n",
    "t = t[p]\n",
    "print(x.shape, t.shape)\n",
    "\n",
    "# Particion entrenamiento-test:\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.33, random_state=42)\n",
    "\n",
    "# NO ME OLVIDO DE NORMALIZAR PARA EXPLICAR\n",
    "medias = x_train.mean(axis=0, keepdims=True)\n",
    "stds = x_train.std(axis=0, keepdims=True)\n",
    "x_train = (x_train-medias)/stds\n",
    "x_test = (x_test-medias)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPpvKDlG9zhu",
    "outputId": "2e8cfcb6-651a-4258-dafc-31487a6ee942"
   },
   "outputs": [],
   "source": [
    "# Voy a utilizar regularización L1 para intentar seleccionar atributos\n",
    "# Ojo: solver=\"saga\" es el único que acepta L1 como penalty en sklearn\n",
    "# La penalización se introduce con un factor C, similar a las SVMs.\n",
    "#   Cuanto menor es C, mayor es la regularización\n",
    "lr = LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.1)\n",
    "lr.fit(x_train, t_train)\n",
    "pred = lr.predict_proba(x_test)\n",
    "lr.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyrEZPUL-YMz"
   },
   "outputs": [],
   "source": [
    "w = lr.coef_\n",
    "b = lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-kkCaMR_Ho7",
    "outputId": "618cca2b-4cda-47b2-d624-60a448add09e"
   },
   "outputs": [],
   "source": [
    "np.isclose(softmax(x_test @ w.T + b), pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3TtdvQhDsal"
   },
   "source": [
    "### Hagamos selección de atributos según los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bV1N0NjD8ar"
   },
   "source": [
    "Intentemos hacer selección de atributos según los pesos de la red...\n",
    "\n",
    "Esperad. Ahora no tenemos un vector de pesos $\\mathbf{w}$, sino que tenemos una matriz de pesos $\\mathbf{W}$. ¿Qué hacemos?\n",
    "\n",
    "**Podríamos intentar hacer selección de atributos según los pesos para cada clase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "SKqGHDKlD11p",
    "outputId": "ff913e14-cc13-4b34-d54b-d6b73d942007"
   },
   "outputs": [],
   "source": [
    "R = np.abs(w.T)\n",
    "\n",
    "for c in range(3): # 3 clases\n",
    "  plt.figure(figsize=(10, 1))\n",
    "  plt.title(iris.target_names[c])\n",
    "  plt.bar(range(len(R)), R[:, c])\n",
    "  plt.xticks(range(len(R)), iris.feature_names)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo-qef96Ep-V"
   },
   "source": [
    "Como podemos ver, **si fijamos bien la regularización**, se está fijando en un atributo distinto para cada clase, y el resto de pesos los está poniendo a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D70v6QcVGJNU"
   },
   "source": [
    "### Hagamos explicabilidad del modelo según la derivada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9euRPNgpGTn3"
   },
   "source": [
    "Aún no lo sabemos (*lo veremos en la próxima clase con redes neuronales con capas ocultas*), pero la derivada de un modelo de regresión logística es la misma derivada que la regresión lineal (*se están asumiendo ciertos detalles que veremos más adelante*).\n",
    "\n",
    "Recordemos del día anterior. **¿Qué representa exactamente la derivada de una función??**\n",
    "\n",
    "La derivada de una función $\\frac{\\partial L(y, t)}{\\partial x_i}$ representa la tasa de cambio de la función de coste respecto a $x_i$, es decir, cómo varía la función de coste cuando varía ligeramente $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7App1ZcERYfZ"
   },
   "source": [
    "Como ya vimos, los algoritmos de explicabilidad específica de redes neuronales se suelen basar en la técnica del gradiente respecto a los atributos de entrada (**Gradient x Input**). Si quieres explicar qué tiene que pasar en la entrada para que la predicción del modelo se acerque al valor real:\n",
    "\n",
    "$$R_{\\mathbf{x}} = - |\\mathbf{x}| \\cdot \\nabla_\\mathbf{x}L(\\mathbf{y}, \\mathbf{t})$$\n",
    "\n",
    "o si quieres explicar cómo se va a comportar el modelo para cada clase:\n",
    "\n",
    "$$R_{\\mathbf{x}}^c = |\\mathbf{x}| \\cdot \\nabla_\\mathbf{x}(y_c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccmYvg_vCool"
   },
   "outputs": [],
   "source": [
    "def grad_x_input(x, model, t, explain_target=False):\n",
    "    t = (t[:, None] == np.arange(3)[None, :])*1.0\n",
    "    y = model.predict_proba(x)\n",
    "    w = model.coef_\n",
    "    dL_dy = (y-t)\n",
    "    dy_dx = w\n",
    "    dL_dx = dL_dy @ dy_dx\n",
    "\n",
    "    if explain_target:\n",
    "        R = -np.abs(x) * dL_dx\n",
    "    else:\n",
    "        R = np.array([np.abs(x) * dy_dx[i][None, :] for i in range(3)])\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDxONePRRzMv"
   },
   "source": [
    "#### Explicar el comportamiento del modelo según la salida esperada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nydhb4CfR6NH"
   },
   "source": [
    "Podemos visualizar el comportamiento del modelo para analizar su salida respecto a la clase real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Keia6BfEGnWS",
    "outputId": "ab0d699c-722b-4411-ef51-15e0ad0e3f90"
   },
   "outputs": [],
   "source": [
    "R = grad_x_input(x_train, lr, t_train, explain_target=True)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "5vi82vUqGqg1",
    "outputId": "7437966f-5859-4b69-c93c-4a9dd1f4c682"
   },
   "outputs": [],
   "source": [
    "item = 0 # Modificar esta variable para evaluar otros ejemplos\n",
    "\n",
    "ypred_real = lr.predict_proba(x_train[item][None, :])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(x_train.shape[1]), x_train[item])\n",
    "plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.subplot(1,2,2)\n",
    "for c in range(3):\n",
    "  plt.plot(c, ypred_real[0, c], 'o', label=\"Predicción \"+str(iris.target_names[c]))\n",
    "plt.xticks(range(3), iris.target_names)\n",
    "plt.grid()\n",
    "plt.title(\"Clase real: \" + iris.target_names[t_train[item]])\n",
    "plt.ylabel(\"Probabilidad asignada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "w0RWOw-dHYeE",
    "outputId": "646c9cab-f164-498d-ad51-b39c9247e404"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(x_train.shape[1]), R[item], color=\"red\")\n",
    "plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vlla9R2IcD8"
   },
   "outputs": [],
   "source": [
    "def dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(range(x_train.shape[1]), x_train[item], alpha=0.5)\n",
    "    plt.bar(range(x_train.shape[1]), x_train_modificado, color=\"magenta\", alpha=0.5)\n",
    "    plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.subplot(1,2,2)\n",
    "    for c in range(3):\n",
    "      plt.plot(c, ypred_real[0, c], 'o', label=\"Predicción real \"+str(iris.target_names[c]))\n",
    "      plt.plot(c, ypred[0, c], 'o', label=\"Predicción modificada \"+str(iris.target_names[c]))\n",
    "      plt.annotate('', xy=(c, ypred[0, c]), xytext=(c, ypred_real[0, c]), arrowprops=dict(arrowstyle='->', color=\"black\"))\n",
    "    plt.xticks(range(3), iris.target_names)\n",
    "    plt.ylabel(\"Probabilidad asignada\")\n",
    "    plt.title(\"Clase real: \" + iris.target_names[t_train[item]])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqabLrT4J1rz"
   },
   "outputs": [],
   "source": [
    "def explicabilidad(item, x_train, R, model):\n",
    "    print(\" >> La relevancia de los atributos es la siguiente:\")\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(range(x_train.shape[1]), R[item], color=\"red\")\n",
    "    plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if (R[item] > 0).any():\n",
    "      print(\" >> Si disminuyo un atributo con relevancia positiva -> la probabilidad de la clase real \"+str(iris.target_names[t_train[item]])+\" baja\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] - 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "      print(\" >> Si aumento un atributo con relevancia positiva -> la probabilidad de la clase real \"+str(iris.target_names[t_train[item]])+\" sube\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[item] > 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] + 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "    if (R[item] < 0).any():\n",
    "      print(\" >> Si disminuyo un atributo con relevancia negativa -> la probabilidad de la clase real \"+str(iris.target_names[t_train[item]])+\" sube\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] - 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "      print(\" >> Si aumento un atributo con relevancia negativa -> la probabilidad de la clase real \"+str(iris.target_names[t_train[item]])+\" baja\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[item] < 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] + 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TZe2bFOcIDmI",
    "outputId": "bb20d76a-b2db-4445-fcf0-c5e2f6688c1e"
   },
   "outputs": [],
   "source": [
    "explicabilidad(item, x_train, R, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwIaRokGOHs0"
   },
   "source": [
    "#### Explicar la predicción de cada salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW7ueZhcSnR0"
   },
   "source": [
    "Podemos explicar el modelo para cada salida individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "_oXOnK5KOUSo",
    "outputId": "02e050d7-a197-41d0-ca1f-ba2eef346876"
   },
   "outputs": [],
   "source": [
    "item = 0 # Modificar esta variable para evaluar otros ejemplos\n",
    "\n",
    "ypred_real = lr.predict_proba(x_train[item][None, :])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(x_train.shape[1]), x_train[item])\n",
    "plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.subplot(1,2,2)\n",
    "for c in range(3):\n",
    "  plt.plot(c, ypred_real[0, c], 'o', label=\"Predicción \"+str(iris.target_names[c]))\n",
    "plt.xticks(range(3), iris.target_names)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZZrL22VORO-",
    "outputId": "85bf740c-3704-4e5b-83d3-255ccbbfa2e1"
   },
   "outputs": [],
   "source": [
    "R = grad_x_input(x_train, lr, t_train, explain_target=False)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "lHnmQu2dJm3F",
    "outputId": "3fec769b-e2f1-4872-facc-48430cf2e7b7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(3):\n",
    "  plt.subplot(1,3,i+1)\n",
    "  plt.bar(range(x_train.shape[1]), R[i, item], color=\"red\")\n",
    "  plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "  plt.grid(alpha=0.2)\n",
    "  plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irowQ-i0OVwW"
   },
   "outputs": [],
   "source": [
    "def dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(range(x_train.shape[1]), x_train[item], alpha=0.5)\n",
    "    plt.bar(range(x_train.shape[1]), x_train_modificado, color=\"magenta\", alpha=0.5)\n",
    "    plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.subplot(1,2,2)\n",
    "    for c in range(3):\n",
    "      plt.plot(c, ypred_real[0, c], 'o', label=\"Predicción real \"+str(iris.target_names[c]))\n",
    "      plt.plot(c, ypred[0, c], 'o', label=\"Predicción modificada \"+str(iris.target_names[c]))\n",
    "      plt.annotate('', xy=(c, ypred[0, c]), xytext=(c, ypred_real[0, c]), arrowprops=dict(arrowstyle='->', color=\"black\"))\n",
    "    plt.xticks(range(3), iris.target_names)\n",
    "    plt.ylabel(\"Probabilidad asignada\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LGmJoyjObyc"
   },
   "outputs": [],
   "source": [
    "def explicabilidad(item, x_train, R, model, class_to_analyze):\n",
    "    print(\" >> VAS A EXPLICAR LA CLASE <\" + iris.target_names[class_to_analyze] + \">\")\n",
    "    print(\" >> La relevancia de los atributos es la siguiente:\")\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(range(x_train.shape[1]), R[class_to_analyze, item], color=\"red\")\n",
    "    plt.xticks(range(x_train.shape[1]), iris.feature_names, rotation=45)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.axhline(0.0, color=\"gray\", alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if (R[class_to_analyze, item] > 0).any():\n",
    "      print(\" >> Si disminuyo un atributo con relevancia positiva para la clase \"+str(iris.target_names[class_to_analyze])+\" -> la probabilidad de la clase predicha \"+str(iris.target_names[class_to_analyze])+\" baja\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[class_to_analyze, item] > 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] - 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "      print(\" >> Si aumento un atributo con relevancia positiva para la clase \"+str(iris.target_names[class_to_analyze])+\" -> la probabilidad de la clase predicha \"+str(iris.target_names[class_to_analyze])+\" sube\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[class_to_analyze, item] > 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] + 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "    if (R[class_to_analyze, item] < 0).any():\n",
    "      print(\" >> Si disminuyo un atributo con relevancia negativa para la clase \"+str(iris.target_names[class_to_analyze])+\" -> la probabilidad de la clase predicha \"+str(iris.target_names[class_to_analyze])+\" sube\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[class_to_analyze, item] < 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] - 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)\n",
    "\n",
    "      print(\" >> Si aumento un atributo con relevancia negativa para la clase \"+str(iris.target_names[class_to_analyze])+\" -> la probabilidad de la clase predicha \"+str(iris.target_names[class_to_analyze])+\" baja\")\n",
    "      x_train_modificado = x_train[item].copy()\n",
    "      i_columna = np.random.choice(np.where(R[class_to_analyze, item] < 0)[0])\n",
    "      x_train_modificado[i_columna] = x_train_modificado[i_columna] + 0.2\n",
    "      ypred = model.predict_proba(x_train_modificado[None, :])\n",
    "      ypred_real = model.predict_proba(x_train[item][None, :])\n",
    "      dibuja_figura(item, x_train, x_train_modificado, ypred_real, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A36FGKQaO6AM",
    "outputId": "e5fea119-8609-4f55-cdf7-8e4877682c07"
   },
   "outputs": [],
   "source": [
    "explicabilidad(item, x_train, R, lr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkH-ra6KPCHJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
